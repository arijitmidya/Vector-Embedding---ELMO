{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBi12NcHzOR7",
    "outputId": "6344632d-fb7b-42db-c0d4-79f978668855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kUHY9NEfNBk",
    "outputId": "027c6513-9ad5-49ab-b273-7770a278ff7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETCnIDEmXjIF",
    "outputId": "14d28e9c-19fe-497c-b43b-898afb33f7c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "WOlwuUt7zXWm"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sf9vRhNlMvk"
   },
   "outputs": [],
   "source": [
    "# Load the ELMO model\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59OyQpaVRqal"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "\n",
    "    \"\"\"Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_file: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the extracted text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    with open(pdf_file, 'rb') as pdf_reader:\n",
    "        reader = PyPDF2.PdfReader(pdf_reader)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjPAPhIjfZfs"
   },
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "\n",
    "pdf_file = \"/content/sample_data/Comparativestudyofwordembeddingalgorithm.pdf\"\n",
    "text = extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "Vj70N_6afwhc",
    "outputId": "d952339e-bb08-46dc-9a26-95472b14d76e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 112 (2017)  340–349\\n1877-0509 © 2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International\\n10.1016/j.procs.2017.08.009\\n10.1016/j.procs.2017.08.009© 2017 The Authors. Published by Elsevier B.V .\\nPeer-review under responsibility of KES International\\n1877-0509Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords: Word embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords: Word embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.independent from the corpus size. Baroni et al.6proved that Word2Vec outperforms traditional distributional methods\\nlike pointwise mutual information (PMI). Likewise, Levy et al.3proved that Word2Vec (Skip Grams with Negative\\nSampling SGNS) outperforms GloVe in many tasks such as word similarity. To explain these results, they said that\\nPennington et al.2only used Google’s analogies for the evaluation.\\nHence, in this paper, we will study LSA, Word2Vec and GloVe to determine which one is the most e ﬃcient method\\nfor learning vector representation. Moreover, this study will be done in the ﬁeld of topic segmentation. In fact, best to\\nour knowledge, no one has used Word2vec or GloVe in this ﬁeld. Furthermore, this paper exploits the bilingual aspect\\nof these methods by focusing on two languages: English and Arabic.\\nThis paper is organized as follows: Section 2 presents related works in the ﬁeld of topic segmentation; Section 3\\ndescribes LSA, Word2Vec and GloVe; Section 4 presents the proposed topic segmenter; In section 5, experimental\\nresults and discussion are reported; The conclusion and future work are presented in section 6.\\n2. Overview on topic segmentation\\nTopic segmentation is the process of dividing a document into coherent segments, such as each segment deals with\\na speciﬁc topic. For the last years, many topic segmenters have been presented and they can be classiﬁed according to\\ntwo approaches: endogenous and exogenous approaches. For the endogenous approach, the process of topic segmen-\\ntation is based only on information within the documents to be segmented. Yet, for the exogenous approach, external\\nresources can be used in the process of topic segmentation in order to add external knowledge. Most of the proposed\\ntopic segmenters are used for English language. The most known segmenters are TextTiling7and C998which are\\nconsidered as endogenous topic segmenters. These segmenters are based on lexical repetition. Other examples of en-dogenous topic segmenters are LCseg\\n9, F0610and TopicTiling11. LCseg9is a lexical cohesion segmenter which uses\\nlexical chain repetitions to detect topic boundaries based on cohesion function. F0610and TopicTiling11are based\\non TextTiling7. One of the di ﬀerences is that F0610measures the similarity between sentences with Dice metric and\\nnot Cosine metric like in TextTiling7. Yet TopicTiling10calculates the similarity between blocs based on topic vector\\nrepresentations, which are constructed by LDA model, and the cosine measure. On the other hand, several exogenous\\ntopic segmenters are proposed based on diﬀ erent external resources such as: LSA12, co-occurrence network10and\\ngenerative Bayesian model13.\\nYet for the Arabic language, there is a ﬂagrant lack of research in this ﬁeld. Only few works deal with endogenous\\ntopic segmentation such as the work of Habacha et al.14. They adapted C99 and TextTiling to Arabic by using Khoja\\nStemmer. On the other hand, only two works deal with exogenous topic segmentation15,16. Brants et al.15used the\\nProbabilistic Latent Semantic Analysis PLSA to propose a new Arabic topic segmenter named TopSeg. Touir et al.16\\nalso proposed a new topic segmenter based on a list of connectors as an external resource.\\nBased on these related works in the ﬁeld of topic segmentation, we notice that only classical methods are used\\nsuch as LSA12, PLSA15and LDA10. Yet, recently, other modern methods have been proposed and they have showed\\npromising results such as Word2Vec and GloVe. We also can notice that there is a lack of exogenous topic segmentersfor the Arabic Language. For these reasons, we will propose exogenous topic segmenters for both English and Arabic\\nlanguages. As external resources, we will use LSA as a traditional method and Word2Vec and Glove as modern\\nmethods. Moreover, we will study in depth which of these methods o ﬀers the best word representations that help to\\ndetect the semantic meaning of words.\\n3. Overview on LSA Word2vec and Glove\\nA word is well described by its context. This idea presents the main principle of LSA, Word2Vec and GloVe.\\nHowever, the process of each method is diﬀerent from the other.\\n•Latent Semantic Analysis: LSA\\n17is a powerful statistical technique. It is based on two main steps. The ﬁrst\\nstep corresponds to the construction of a term-document matrix M. The size of Misn∗mwhere the rows\\ncorrespond to m terms, the columns correspond to n document and M[i,j] corresponds to the frequency of the Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 341 Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.\\nKeywords:\\nWord embedding, LSA, Word2Vec, GloVe, Topic segmentation.\\n1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3.\\nHowever, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al.\\n2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel.: +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.independent from the corpus size. Baroni et al.6proved that Word2Vec outperforms traditional distributional methods\\nlike pointwise mutual information (PMI). Likewise, Levy et al.3proved that Word2Vec (Skip Grams with Negative\\nSampling SGNS) outperforms GloVe in many tasks such as word similarity. To explain these results, they said that\\nPennington et al.2only used Google’s analogies for the evaluation.\\nHence, in this paper, we will study LSA, Word2Vec and GloVe to determine which one is the most e ﬃcient method\\nfor learning vector representation. Moreover, this study will be done in the ﬁeld of topic segmentation. In fact, best to\\nour knowledge, no one has used Word2vec or GloVe in this ﬁeld. Furthermore, this paper exploits the bilingual aspect\\nof these methods by focusing on two languages: English and Arabic.\\nThis paper is organized as follows: Section 2 presents related works in the ﬁeld of topic segmentation; Section 3\\ndescribes LSA, Word2Vec and GloVe; Section 4 presents the proposed topic segmenter; In section 5, experimental\\nresults and discussion are reported; The conclusion and future work are presented in section 6.\\n2. Overview on topic segmentation\\nTopic segmentation is the process of dividing a document into coherent segments, such as each segment deals with\\na speciﬁc topic. For the last years, many topic segmenters have been presented and they can be classiﬁed according to\\ntwo approaches: endogenous and exogenous approaches. For the endogenous approach, the process of topic segmen-\\ntation is based only on information within the documents to be segmented. Yet, for the exogenous approach, external\\nresources can be used in the process of topic segmentation in order to add external knowledge. Most of the proposed\\ntopic segmenters are used for English language. The most known segmenters are TextTiling7and C998which are\\nconsidered as endogenous topic segmenters. These segmenters are based on lexical repetition. Other examples of en-dogenous topic segmenters are LCseg\\n9, F0610and TopicTiling11. LCseg9is a lexical cohesion segmenter which uses\\nlexical chain repetitions to detect topic boundaries based on cohesion function. F0610and TopicTiling11are based\\non TextTiling7. One of the di ﬀerences is that F0610measures the similarity between sentences with Dice metric and\\nnot Cosine metric like in TextTiling7. Yet TopicTiling10calculates the similarity between blocs based on topic vector\\nrepresentations, which are constructed by LDA model, and the cosine measure. On the other hand, several exogenous\\ntopic segmenters are proposed based on diﬀ erent external resources such as: LSA12, co-occurrence network10and\\ngenerative Bayesian model13.\\nYet for the Arabic language, there is a ﬂagrant lack of research in this ﬁeld. Only few works deal with endogenous\\ntopic segmentation such as the work of Habacha et al.14. They adapted C99 and TextTiling to Arabic by using Khoja\\nStemmer. On the other hand, only two works deal with exogenous topic segmentation15,16. Brants et al.15used the\\nProbabilistic Latent Semantic Analysis PLSA to propose a new Arabic topic segmenter named TopSeg. Touir et al.16\\nalso proposed a new topic segmenter based on a list of connectors as an external resource.\\nBased on these related works in the ﬁeld of topic segmentation, we notice that only classical methods are used\\nsuch as LSA12, PLSA15and LDA10. Yet, recently, other modern methods have been proposed and they have showed\\npromising results such as Word2Vec and GloVe. We also can notice that there is a lack of exogenous topic segmentersfor the Arabic Language. For these reasons, we will propose exogenous topic segmenters for both English and Arabic\\nlanguages. As external resources, we will use LSA as a traditional method and Word2Vec and Glove as modern\\nmethods. Moreover, we will study in depth which of these methods o ﬀers the best word representations that help to\\ndetect the semantic meaning of words.\\n3. Overview on LSA Word2vec and Glove\\nA word is well described by its context. This idea presents the main principle of LSA, Word2Vec and GloVe.\\nHowever, the process of each method is diﬀerent from the other.\\n•Latent Semantic Analysis: LSA\\n17is a powerful statistical technique. It is based on two main steps. The ﬁrst\\nstep corresponds to the construction of a term-document matrix M. The size of Misn∗mwhere the rows\\ncorrespond to m terms, the columns correspond to n document and M[i,j] corresponds to the frequency of the342 Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349\\nterm i in the document j. The second step is the singular value decomposition where Mwill be decomposed,\\naccording to the equation 1, into three matrices: U,VTwhich are two orthogonal matrices and Swhich is\\na diagonal matrix. Finally, based on equation 2, only the klargest singular values and their corresponding\\nsingular vectors from UandVTwill be used in order to reduce the semantic space which corresponds to Mk.\\nM=U∗S∗VT(1)\\nMk=Uk∗Sk∗Vt\\nk (2)\\nThe LSA method is based on several parameters which are: local and global frequencies setting, local and\\nglobal weighting functions and the dimension of the semantic space. Naili et al.12have conducted an empirical\\nstudy of LSA parameters in the ﬁeld of topic segmentation. As result, they proved that these parameters have an\\nimportant impact on the quality of topic segmentation. Furthermore, the best choices are: local frequency =3,\\nglobal frequency =1, local weighting function is TF, Global weighting function is IDF and the dimension of the\\nreduced semantic space is equal to 70%. In this paper, we will use these same choices as Naili et al.12for LSA.\\n•Word2Vectors: Word2Vec is a shallow word embedding model proposed by Mikolov et al.1. The main princi-\\nple of this method is to learn law dimensional vectors from the begging. In fact, it predicts words based on their\\ncontext by using one of two distinct neural models: CBOW and Skip-Gram.\\nContinuous bag of words (CBOW) predicts a current word based on its context. This latter corresponds to\\nthe neighboring words in the window. In the process of CBOW, three layers are used. The input layer corre-\\nsponds to the context. The hidden layer corresponds to the projection of each word from the input layer into\\nthe weight matrix which is projected into the third layer which is the output layer. The ﬁnal step of this model\\nis the comparison between its output and the word itself in order to correct its representation based on the back\\npropagation of the error gradient. Thus, the purpose of CBOW neural network is to maximize the following\\nequation 3:\\n1\\nVV∑\\nt=1logp(m t|mt−c\\n2...m t+c\\n2) (3)\\nwhere V corresponds to vocabulary size, c corresponds to the window size of each word.\\nSkip-Gram: it is the opposite of the CBOW models. In fact, the input layer corresponds to the target word\\nand the output layer corresponds to the context. Thus, Skip-Gram seeks the prediction of the context given\\na word instead of the prediction of a word given its context like CBOW. The ﬁnal step of Skip-Gram is the\\ncomparison between its output and each word of the context in order to correct its representation based on theback propagation of the error gradient. In fact, it seeks the maximization of the following equation 4:\\n1\\nVV∑\\nt=1t+c∑\\nj=t−c,j/nequaltlogp(mj|mt) (4)\\nwhere V corresponds to vocabulary size, c corresponds to the window size of each word.According to Mikolov et al.\\n1, each one of these models has its own advantage. As example, Skip-Gram is more\\neﬃcient with small training data. Moreover, infrequent words are well presented. On the other hand, CBOWis faster and works well with frequent words. However, learning the output vectors of CBOW and Skip-Gram\\nrepresents one of the major limits of these two models. In fact, learning the output vectors can be a hard and\\nan expensive task. In this context, two algorithms can be used to address this problem. The ﬁrst algorithm is\\nthe Negative Sampling algorithm. The main idea of this algorithm is to limit the number of output vectors that\\nneed to be updating. Thus, only a sample of these vectors is updated based on a noise distribution. This latter\\nis a probabilistic distribution which is used in the sampling process. The second algorithm is the Hierarchical\\nSoftmax. This algorithm is based on Hu ﬀman tree. In fact, it is a binary tree that presents all terms based\\non their frequencies. Then each step from the root to the target is normalized. According to Mikolov et al.\\n1,\\neach algorithm is better than the other according to the training data. For example, negative sampling is more\\neﬃcient with low dimensional vectors and it works better with frequent words. Yet, hierarchical softmax is\\nbetter with infrequent words. As conclusion, using Word2Vec can be a hard and a complicate task considering\\nthe diﬀ erent models (CBOW and Skip-Gram) and the used algorithms for training data (negative sampling andhierarchical softmax). Thus in this paper, we will investigate the performance of these di ﬀerent models and\\nalgorithms to determine which ones are more eﬃcient in the domain of topic segmentation.\\n•Global Vectors: GolVe is one of the most known methods for learning word representations proposed by\\nPennington et al.2. It is based on word occurrences in a textual corpus. In fact, it is based on two main steps.\\nThe ﬁrst one is the construction of a co-occurrence matrix X from a training corpus where:\\nXijis the frequency of the word ico-occurring with the word j\\nXij=∑V\\nkXikis the total number of occurrences of the word iin the corpus (V corresponds to the size of the\\ncorpus)\\nThe second step is the factorization of X in order to get vectors. In fact, Pennington et al.2showed that,\\ncompared to raw probabilities, ratios help to reduce noise by identifying relevant words form irrelevant words.For this reason, they used the following general model (equation 5):\\nF(w\\ni−wj,˜wk)=Pik\\nPjk(5)\\nwhere wi,wjandwkare three words’ vector, Pik=Xik/Xiis the probability of the word koccurring within the\\ncontext of the word i,ware word vectors and ˜ wkare context word vectors.\\nYet to preserve the linearity and prevent mixing dimensions, Pennington et al.2used vector diﬀerences and the\\ndot product of the arguments in equation 5 and it becomes:\\nF((w i−wj)T˜wk)=Pik\\nPjk(6)\\nHowever, the ﬁnal model should be unchanged by exchanging w−>˜wandX−>XT. To resolve this symmetry,\\nequation 6 becomes as follow:\\nwT\\ni˜wk+bi+˜bk=log(Xik) (7)\\nFinally, Pennington et al.2proposed a least squares objective function by equation 8 where f(x) is a weighting\\nfunction:\\nJ=V∑\\ni,j=1f(Xij)(wTi˜wj+bi+˜bj−log(Xij))2(8)\\n4. Proposed topic segmenter\\nTo study the performance of LSA, Word2Vec and GloVe in topic segmentation, we use ToSe-LSA which is pro-\\nposed by Naili et al.12. This segmenter is based on LSA to construct a semantic space which is used as an external\\nresource. Therefore, the main idea is to employ diﬀ erent semantic spaces in order to ﬁnd which method is more\\neﬀective for learning vector representation of words to capture their semantic meaning. As shown in Fig. 1, the topic\\nsegmenter ToSe is based on ﬁve steps. The ﬁrst one is the pre-processing step which allows the following operations:\\ndetection of language, extraction of words, elimination of stop words, stemming the remaining words. The second\\nstep is the frequency dictionary construction. This dictionary is composed of vectors. Each vector is associated to a\\nsentence and it is composed of: terms, their corresponding stems, frequencies and vectors. We note that these latter\\nare extracted from the semantic space if it exists. If the term does not belong to the semantic space, it is presented by\\na null vector. The third step is the similarity matrix construction. In this step, we calculate the similarity between all\\npairs of all terms that belong to the semantic space. Then, we employ equation (9) to calculate the similarity between\\nsentences. The forth step allows the construction of a rank matrix. We note that the rank presents the number of\\nneighboring elements that belong to the rank mask and have lower values of similarity. The ﬁnal step is dedicated to\\ndetect topic boundaries based on Reynar’s algorithm.\\nS im(S 1,S2)=∑\\nti∈S1∩SS∑\\ntj∈S2∩SS(FtiFtjcos(Vt i,Vtj))\\n∑\\nti∈S1(Fti)∑\\ntj∈S2(Ftj)(9)\\nWith S1andS2correspond to sentences 1 and 2; SScorresponds to the Semantic Space; FtiandFtjcorrespond\\nto the frequency of terms tiandtj;VtiandVtjcorrespond to the vectors of tiandtjinSS. Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 343\\nhierarchical softmax). Thus in this paper, we will investigate the performance of these di ﬀerent models and\\nalgorithms to determine which ones are more eﬃcient in the domain of topic segmentation.\\n•Global Vectors: GolVe is one of the most known methods for learning word representations proposed by\\nPennington et al.2. It is based on word occurrences in a textual corpus. In fact, it is based on two main steps.\\nThe ﬁrst one is the construction of a co-occurrence matrix X from a training corpus where:\\nXijis the frequency of the word ico-occurring with the word j\\nXij=∑V\\nkXikis the total number of occurrences of the word iin the corpus (V corresponds to the size of the\\ncorpus)\\nThe second step is the factorization of X in order to get vectors. In fact, Pennington et al.2showed that,\\ncompared to raw probabilities, ratios help to reduce noise by identifying relevant words form irrelevant words.For this reason, they used the following general model (equation 5):\\nF(w\\ni−wj,˜wk)=Pik\\nPjk(5)\\nwhere wi,wjandwkare three words’ vector, Pik=Xik/Xiis the probability of the word koccurring within the\\ncontext of the word i,ware word vectors and ˜ wkare context word vectors.\\nYet to preserve the linearity and prevent mixing dimensions, Pennington et al.2used vector diﬀerences and the\\ndot product of the arguments in equation 5 and it becomes:\\nF((w i−wj)T˜wk)=Pik\\nPjk(6)\\nHowever, the ﬁnal model should be unchanged by exchanging w−>˜wandX−>XT. To resolve this symmetry,\\nequation 6 becomes as follow:\\nwT\\ni˜wk+bi+˜bk=log(Xik) (7)\\nFinally, Pennington et al.2proposed a least squares objective function by equation 8 where f(x) is a weighting\\nfunction:\\nJ=V∑\\ni,j=1f(Xij)(wTi˜wj+bi+˜bj−log(Xij))2(8)\\n4. Proposed topic segmenter\\nTo study the performance of LSA, Word2Vec and GloVe in topic segmentation, we use ToSe-LSA which is pro-\\nposed by Naili et al.12. This segmenter is based on LSA to construct a semantic space which is used as an external\\nresource. Therefore, the main idea is to employ diﬀ erent semantic spaces in order to ﬁnd which method is more\\neﬀective for learning vector representation of words to capture their semantic meaning. As shown in Fig. 1, the topic\\nsegmenter ToSe is based on ﬁve steps. The ﬁrst one is the pre-processing step which allows the following operations:\\ndetection of language, extraction of words, elimination of stop words, stemming the remaining words. The second\\nstep is the frequency dictionary construction. This dictionary is composed of vectors. Each vector is associated to a\\nsentence and it is composed of: terms, their corresponding stems, frequencies and vectors. We note that these latter\\nare extracted from the semantic space if it exists. If the term does not belong to the semantic space, it is presented by\\na null vector. The third step is the similarity matrix construction. In this step, we calculate the similarity between all\\npairs of all terms that belong to the semantic space. Then, we employ equation (9) to calculate the similarity between\\nsentences. The forth step allows the construction of a rank matrix. We note that the rank presents the number of\\nneighboring elements that belong to the rank mask and have lower values of similarity. The ﬁnal step is dedicated to\\ndetect topic boundaries based on Reynar’s algorithm.\\nS im(S 1,S2)=∑\\nti∈S1∩SS∑\\ntj∈S2∩SS(FtiFtjcos(Vt i,Vtj))\\n∑\\nti∈S1(Fti)∑\\ntj∈S2(Ftj)(9)\\nWith S1andS2correspond to sentences 1 and 2; SScorresponds to the Semantic Space; FtiandFtjcorrespond\\nto the frequency of terms tiandtj;VtiandVtjcorrespond to the vectors of tiandtjinSS.344 Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349\\nThe main originality of this segmenter is that it can be used for diﬀerent languages. In this paper, we are limited\\nto English and Arabic languages. For the English language, the stemming process is conducted by Porter Stemmer.\\nYet for the Arabic language, the stemming process is based on Light10 Stemmer. Moreover, based on the method that\\nwill be used to construct the semantic space, we propose three topic segmenters: ToSe-LSA (the original segmenter),\\nToSe-Word2Vec and ToSe-GloVe.\\nFig. 1. Topic segmentation process\\n5. Evaluation and discussion\\nTo determine which method of LSA, Word2Vec and GloVe is better to produce word representations, we conduct\\nan evaluation based on two corpora. The ﬁrst one is an English corpus which contains scientiﬁc articles from ACM\\ndigital library. These articles deal with diﬀerent computer science topics. The second corpus is an Arabic benchmark.\\nThis latter is proposed by Al-Sulaiti and it deals with diﬀerent topics such as: economy, politic, health and science.\\nBased on these corpora, we construct two collections for each language: training collection and a test collection. The\\ntraining collection is used to construct semantic spaces. The test collection is used in order to compare LSA, Word2Vec\\nand GloVe in the ﬁeld of topic segmentation and it contains artiﬁcial documents. These latter are constructed from a\\nserial concatenation of documents. To report the evaluation results, we use the WindowDi ﬀmetric that measures the\\nerror rate by using a sliding window. Moreover, to conduct these studies we used : the R package LSA, Word2Vec\\ntool proposed by Mikolov et al.1and the R text2vec package for GloVe.\\nThe ﬁrst part of this evaluation is dedicated to study Word2Vec in the domain of topic segmentation. Then we in-\\nvestigate the performance of LSA, Word2Vec and GloVe. Finally, we compare the proposed segmenters with existent\\ntopic segmenters.\\n5.1. Evaluation of Word2Vec parameters\\nTable 1 presents the average results of WindowDi ﬀfor Word2Vec based on diﬀerent learning models and algo-\\nrithms and the two languages English and Arabic. For the English language, there is an important di ﬀerence between\\nthe performances of each combination of models and approximation algorithms. For CBOW, using the negative sam-\\npling is slightly better than the hierarchical softmax. Yet for Skip-Gram, hierarchical softmax is much better than the\\nnegative sampling algorithm. However, independent from the used algorithm for optimization, the performance ofCBOW is much better than Skip-Gram. For the Arabic language, the performance of each combination is very close.\\nHowever, independent from the learning model, the negative sampling algorithm is more e ﬀective than hierarchical\\nsoftmax algorithms. On the other hand, Skip-Gram is more e ﬃcient than CBOW.\\nBased on this evaluation, we can say that the choice of the learning model depends on the nature of the training\\ndata which conﬁrms the statement that negative sampling works better with low dimensional vectors as claimed byMikolov et al.\\n1. In fact, for the English language, the training data is speciﬁc to the topic of computer science. In this\\ncase, we are dealing with frequent words. Thus, CBOW is more e ﬃcient than Skip-Gram for the English language.\\nYet for the Arabic language, the training data deals with di ﬀerent topics. In this case, we are dealing with infrequent\\nwords. For this reason Skip-Gram is more e ﬃcient than CBOW for the Arabic language. Another remark is the\\nhigh performance of CBOW especially with the negative sampling algorithms for the English language. This can beexplained by the fact that negative sampling work better with low dimensional vectors as claimed by Mikolov et al.\\n1.\\nIn our case, the dimension of vectors is equal to 100 which is a low dimensional vector.\\nFor the rest of this evaluation, we will use the best combination for each language: CBOW with negative sampling for\\nthe English language and Skip-Gram with negative sampling for the Arabic language.\\nTable 1. Average results of WindowDi ﬀfor Word2Vec.\\nLearning Models Approximation algorithms WindowDiﬀ\\nEnglish Arabic\\nCBOWNegative Sampling 5.79% 31.57%\\nHierarchical Softmax 6.93% 32.76%\\nSkip-GramNegative Sampling 11.54% 29.52%\\nHierarchical Softmax 7.67% 30.53%\\n5.2. Comparison between LSA, Word2Vec and GloVe\\nFig. 2 shows the variation of the WindowDi ﬀvalues of each segmenter (ToSe-LSA, ToSe-Wor2Vec and ToSe-\\nGloVe) for the English language. As ﬁrst remark, it is clear that ToSe-LSA has the biggest error rates. Yet theperformance of ToSe-Word2Vec and ToSe-GloVe is very close. This result is conﬁrmed by Table 2 which presents the\\naverage results of each segmenter and the dimension of the semantic space. According to Table 2, ToSe-Word2Vec\\noutperforms the others and yet it has the smallest semantic space. On the other hand, ToSe-GloVe is slightly less\\neﬃcient than ToSe-Word2Vec and it has the biggest semantic space. Finally, ToSe-LSA has the biggest average rate.\\nFig. 2. WindowDi ﬀcurves of ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the English language\\nFig. 3 presents the variation of the WindowDi ﬀvalues of each segmenter (ToSe-LSA, ToSe-Wor2Vec and ToSe-\\nGloVe) for the Arabic language. We can notice that the variations of these segmenters are very close. Yet ToSe-GloVehas the biggest and smallest values of WindowDi ﬀ. These results are conﬁrmed in Table 3 which describes the average Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 345\\nThe main originality of this segmenter is that it can be used for diﬀerent languages. In this paper, we are limited\\nto English and Arabic languages. For the English language, the stemming process is conducted by Porter Stemmer.\\nYet for the Arabic language, the stemming process is based on Light10 Stemmer. Moreover, based on the method that\\nwill be used to construct the semantic space, we propose three topic segmenters: ToSe-LSA (the original segmenter),\\nToSe-Word2Vec and ToSe-GloVe.\\nFig. 1. Topic segmentation process\\n5. Evaluation and discussion\\nTo determine which method of LSA, Word2Vec and GloVe is better to produce word representations, we conduct\\nan evaluation based on two corpora. The ﬁrst one is an English corpus which contains scientiﬁc articles from ACM\\ndigital library. These articles deal with diﬀerent computer science topics. The second corpus is an Arabic benchmark.\\nThis latter is proposed by Al-Sulaiti and it deals with diﬀerent topics such as: economy, politic, health and science.\\nBased on these corpora, we construct two collections for each language: training collection and a test collection. The\\ntraining collection is used to construct semantic spaces. The test collection is used in order to compare LSA, Word2Vec\\nand GloVe in the ﬁeld of topic segmentation and it contains artiﬁcial documents. These latter are constructed from a\\nserial concatenation of documents. To report the evaluation results, we use the WindowDi ﬀmetric that measures the\\nerror rate by using a sliding window. Moreover, to conduct these studies we used : the R package LSA, Word2Vec\\ntool proposed by Mikolov et al.1and the R text2vec package for GloVe.\\nThe ﬁrst part of this evaluation is dedicated to study Word2Vec in the domain of topic segmentation. Then we in-\\nvestigate the performance of LSA, Word2Vec and GloVe. Finally, we compare the proposed segmenters with existent\\ntopic segmenters.\\n5.1. Evaluation of Word2Vec parameters\\nTable 1 presents the average results of WindowDi ﬀfor Word2Vec based on diﬀerent learning models and algo-\\nrithms and the two languages English and Arabic. For the English language, there is an important di ﬀerence between\\nthe performances of each combination of models and approximation algorithms. For CBOW, using the negative sam-\\npling is slightly better than the hierarchical softmax. Yet for Skip-Gram, hierarchical softmax is much better than the\\nnegative sampling algorithm. However, independent from the used algorithm for optimization, the performance ofCBOW is much better than Skip-Gram. For the Arabic language, the performance of each combination is very close.\\nHowever, independent from the learning model, the negative sampling algorithm is more e ﬀective than hierarchical\\nsoftmax algorithms. On the other hand, Skip-Gram is more e ﬃcient than CBOW.\\nBased on this evaluation, we can say that the choice of the learning model depends on the nature of the training\\ndata which conﬁrms the statement that negative sampling works better with low dimensional vectors as claimed byMikolov et al.\\n1. In fact, for the English language, the training data is speciﬁc to the topic of computer science. In this\\ncase, we are dealing with frequent words. Thus, CBOW is more e ﬃcient than Skip-Gram for the English language.\\nYet for the Arabic language, the training data deals with di ﬀerent topics. In this case, we are dealing with infrequent\\nwords. For this reason Skip-Gram is more e ﬃcient than CBOW for the Arabic language. Another remark is the\\nhigh performance of CBOW especially with the negative sampling algorithms for the English language. This can beexplained by the fact that negative sampling work better with low dimensional vectors as claimed by Mikolov et al.\\n1.\\nIn our case, the dimension of vectors is equal to 100 which is a low dimensional vector.\\nFor the rest of this evaluation, we will use the best combination for each language: CBOW with negative sampling for\\nthe English language and Skip-Gram with negative sampling for the Arabic language.\\nTable 1. Average results of WindowDi ﬀfor Word2Vec.\\nLearning Models Approximation algorithms WindowDiﬀ\\nEnglish Arabic\\nCBOWNegative Sampling 5.79% 31.57%\\nHierarchical Softmax 6.93% 32.76%\\nSkip-GramNegative Sampling 11.54% 29.52%\\nHierarchical Softmax 7.67% 30.53%\\n5.2. Comparison between LSA, Word2Vec and GloVe\\nFig. 2 shows the variation of the WindowDi ﬀvalues of each segmenter (ToSe-LSA, ToSe-Wor2Vec and ToSe-\\nGloVe) for the English language. As ﬁrst remark, it is clear that ToSe-LSA has the biggest error rates. Yet theperformance of ToSe-Word2Vec and ToSe-GloVe is very close. This result is conﬁrmed by Table 2 which presents the\\naverage results of each segmenter and the dimension of the semantic space. According to Table 2, ToSe-Word2Vec\\noutperforms the others and yet it has the smallest semantic space. On the other hand, ToSe-GloVe is slightly less\\neﬃcient than ToSe-Word2Vec and it has the biggest semantic space. Finally, ToSe-LSA has the biggest average rate.\\nFig. 2. WindowDi ﬀcurves of ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the English language\\nFig. 3 presents the variation of the WindowDi ﬀvalues of each segmenter (ToSe-LSA, ToSe-Wor2Vec and ToSe-\\nGloVe) for the Arabic language. We can notice that the variations of these segmenters are very close. Yet ToSe-GloVe\\nhas the biggest and smallest values of WindowDi ﬀ. These results are conﬁrmed in Table 3 which describes the average346 Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349\\nTable 2. Average results of WindowDi ﬀof ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the English language.\\nEnglish topic segmenters Semantic space dimension’s WindowDiﬀ\\nToSe-LSA 3997*100 8.14%\\nToSe-Word2Vec 3455*100 5.79%\\nToSe-GloVe 5954*100 5.94%\\nFig. 3. WindowDi ﬀcurves of ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the Arabic language.\\nresults of each segmenter according to its semantic space dimension. Based on Table 3, ToSe-LSA has the biggest\\naverage error rate with the biggest semantic space. On the other hand, the performance of ToSe-Word2Vec is very\\nclose to ToSe-LSA yet the dimension of its semantic space is much smaller. Finally the highest quality of topic\\nsegmentation is oﬀered by ToSe-GloVe.\\nTable 3. Average results of WindowDi ﬀof ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the Arabic language.\\nArabic topic segmenters Semantic space dimension’s WindowDiﬀ\\nToSe-LSA 35790*200 29.9%\\nToSe-Word2Vec 4581*200 29.52%\\nToSe-GloVe 5454*200 28.43%\\nBased on this evaluation, LSA is the less eﬀective method for learning vector representations for both languages\\nArabic and English. On the other hand, we can say that Word2Vec o ﬀers the best vector representations of words for\\nthe English language. With these vectors, the quality of topic segmentation increased. Yet for the Arabic language,GloVe is slightly more eﬀ ective than Word2Vec. However, we can say that, independent from the used language,\\nWord2Vec outperforms GloVe. In fact, the main advantage of Word2Vec is that with a small semantic space, thesemantic meaning of words is successfully detected. On the other hand, the performance of GloVe is similar to\\nWord2Vec but with a much bigger semantic space. Thus, to detect the semantic meaning of words, GloVe needs more\\ninformation than Word2Vec. This can explain the high performance of GloVe for the Arabic language. In fact, the\\nArabic language is known by its wide variety of grammatical forms and its complex morphology. For this reason, to\\ndetect the semantic meaning of words, more information is needed for this language than the English language. As an\\nexample, we present portions of the constructed Arabic semantic spaces in Fig. 4 for each method (LSA, Word2Vec\\nand GloVe). In these portions, we present the same seven words for each semantic space with their corresponding vec-tors. As shown in ﬁg. 4, each method oﬀers a di ﬀerent vector representation of words. Besides, three stems are driven\\nfrom the same root:\\n/char5a/char41/char0b/chard2/charca/charab(scientists), /chard5/charcb/char41/char0b/charab(scientist, world, universe), /chard5/charce/charab(science, aware, ﬂag, knowledge, teach).\\nHowever, despite the fact that these stems are driven from the same root, each one can has a meaning. Moreover, each\\nstem can has several meaning. Furthermore, if we calculate the similarity between the two stems /chard5/charcb/char41/char0b/charab(scientist) and\\nits plural form /char5a/char41/char0b/chard2/charca/charab(scientists) based on the cosine similarity between their corresponding vectors, we have: 0.393\\nbased on LSA, 0.216 based on GloVe and 0.058 based on Word2Vec. In this example, LSA has detected the similarity Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 347\\nTable 2. Average results of WindowDi ﬀof ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the English language.\\nEnglish topic segmenters Semantic space dimension’s WindowDiﬀ\\nToSe-LSA 3997*100 8.14%\\nToSe-Word2Vec 3455*100 5.79%\\nToSe-GloVe 5954*100 5.94%\\nFig. 3. WindowDi ﬀcurves of ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the Arabic language.\\nresults of each segmenter according to its semantic space dimension. Based on Table 3, ToSe-LSA has the biggest\\naverage error rate with the biggest semantic space. On the other hand, the performance of ToSe-Word2Vec is very\\nclose to ToSe-LSA yet the dimension of its semantic space is much smaller. Finally the highest quality of topic\\nsegmentation is oﬀered by ToSe-GloVe.\\nTable 3. Average results of WindowDi ﬀof ToSe-LSA, ToSe-Word2Vec and ToSe-GloVe for the Arabic language.\\nArabic topic segmenters Semantic space dimension’s WindowDiﬀ\\nToSe-LSA 35790*200 29.9%\\nToSe-Word2Vec 4581*200 29.52%\\nToSe-GloVe 5454*200 28.43%\\nBased on this evaluation, LSA is the less eﬀective method for learning vector representations for both languages\\nArabic and English. On the other hand, we can say that Word2Vec o ﬀers the best vector representations of words for\\nthe English language. With these vectors, the quality of topic segmentation increased. Yet for the Arabic language,GloVe is slightly more eﬀ ective than Word2Vec. However, we can say that, independent from the used language,\\nWord2Vec outperforms GloVe. In fact, the main advantage of Word2Vec is that with a small semantic space, thesemantic meaning of words is successfully detected. On the other hand, the performance of GloVe is similar to\\nWord2Vec but with a much bigger semantic space. Thus, to detect the semantic meaning of words, GloVe needs more\\ninformation than Word2Vec. This can explain the high performance of GloVe for the Arabic language. In fact, the\\nArabic language is known by its wide variety of grammatical forms and its complex morphology. For this reason, to\\ndetect the semantic meaning of words, more information is needed for this language than the English language. As an\\nexample, we present portions of the constructed Arabic semantic spaces in Fig. 4 for each method (LSA, Word2Vec\\nand GloVe). In these portions, we present the same seven words for each semantic space with their corresponding vec-tors. As shown in ﬁg. 4, each method oﬀers a di ﬀerent vector representation of words. Besides, three stems are driven\\nfrom the same root:\\n/char5a/char41/char0b/chard2/charca/charab(scientists), /chard5/charcb/char41/char0b/charab(scientist, world, universe), /chard5/charce/charab(science, aware, ﬂag, knowledge, teach).\\nHowever, despite the fact that these stems are driven from the same root, each one can has a meaning. Moreover, each\\nstem can has several meaning. Furthermore, if we calculate the similarity between the two stems /chard5/charcb/char41/char0b/charab(scientist) and\\nits plural form /char5a/char41/char0b/chard2/charca/charab(scientists) based on the cosine similarity between their corresponding vectors, we have: 0.393\\nbased on LSA, 0.216 based on GloVe and 0.058 based on Word2Vec. In this example, LSA has detected the similarity\\nFig. 4. Portions of the constructed semantic spaces.\\nbetween these stems. Yet, if we calculate the similarity between /char49/char2e/chara3(medicine) and/char09/char90/char51/chard3 (disease), we have: -0.0002\\nbased on LSA, 0.26 based on GloVe and -0.095 based on Word2Vec. In this second example, only GloVe have de-\\ntected the similarity between these two stems which belong to the health topic. Thus, based on the complex nature\\nand structure of the Arabic language and compared to other languages, it is di ﬃcult to detect the semantic meaning of\\nwords which also depends on the vector representation of words. As a consequence, the quality of topic segmentation\\ndepends on the used language. In fact, for English language the WindowDi ﬀis less than 9% in contrast for Arabic\\nlanguage the WindowDi ﬀis more than 28%.348 Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349\\nIf we compare our results with related works, some statements can be doubted. For example, Pennington et\\nal.2argue that GloVe performs better than Word2Vec in every task (word analogies, word similarity and named\\nentity recognition). Even Mitra4proved the same results. Yet, we share the same statement of Mikolov et al.1that\\nWord2Vec outperforms other models. Yet, the performance of GloVe is comparable to Word2Vec and they can be\\nconsidered as two powerful methods to learn word vector representations in the domain of topic segmentation. On the\\nother hand, if we compare modern prediction-based embeddings (Word2Vec and GloVe) with traditional count-based\\nmethods (LSA), we share the same claim as Baroni et al.6that LSA is less eﬃcient than the other models for synonym\\ndetection, concept categorization, selection preferences and analogy. Yet, Levy et al.3argue that LSA outperforms\\nWord2Vec and GloVe for word similarity. Moreover, Pennington et al.2argue that LSA outperforms Word2Vec but\\nnot GloVe for word analogy and similarity and named entity recognition tasks. Thus, we can explain these di ﬀerent\\nclaims by the choice of the domain application that inﬂuences the performance of each method.\\n5.3. Comparison with related works in topic segmentation\\nTo evaluate the performance of the proposed segmenters (ToSe-LSA, ToSA-Word2Vec and ToSe-GloVe), we con-\\nduct a comparison with related works in Tables 4 and 5. In fact, for each language (English and Arabic languages), we\\nconduct an evaluation of endogenous and exogenous segmenters on the same corpus. For the English language (Table\\n4), we can state that exogenous topic segmenters (BSeg, ToSe-LSA, ToSA-Word2Vec and ToSe-GloVe) are more\\neﬃcient than endogenous topic segmenters (C99, TextTiling, F06, LCseg and TopicTiling). This claim is explained\\nby the fact that adding external knowledge enhances the quality of topic segmentation. Nevertheless, the question is\\nwhich external resource that improves better the topic segmentation? In this study, we ﬁnd out that ToSe-LSA which\\nis based on a traditional count-based method has the biggest error rate. Yet the generative Bayesian model of Bseg13,\\nWord2Vec and GloVe are the most eﬃcient methods to use in topic segmentation especially Word2Vec. This can beexplained by the fact that probabilistic models and neural models can detect easily the semantic meaning of words\\nwhich is not the case for traditional methods such as LSA. For the Arabic language (Table 5), we only compared our\\nwork with the work of Habacha et al.\\n14. As shown in Table 5, we noticed that ArabTextTiling and ArabC99, which are\\nbased on endogenous approach, have the biggest error rate. Yet the performance of ToSe-LSA and ToSe-Word2Vec is\\nsimilar. Moreover, ToSe-GloVe is the most e ﬃcient segmenter.\\nTable 4. Comparison with existent English topic segmenters.\\nApproach English topic segmenter WindowDiﬀ\\nEndogenous C99818.92%\\nTextTiling728.99%\\nF061079.22%\\nLCseg913.32%\\nTopicTiling1130.46%\\nExogenous Bseg136.76%\\nToSe-LSA 8.14%\\nToSe-Word2Vec 5.79%\\nToSe-GloVe 5.94%\\nTable 5. Comparison with existent Arabic topic segmenters.\\nApproach Arabic topic segmenter WindowDiﬀ\\nEndogenous ArabC991436.10%\\nArabTextTiling1482.5%\\nExogenous ToSe-LSA 29.9%\\nToSe-Word2Vec 29.52%\\nToSe-GloVe 28.43%\\nBased on this evaluation, we can conclude that exogenous topic segmenters are much way better than endogenous\\ntopic segmenters for both Arabic and English languages. This can be explained by the fact that adding externalknowledge enhances the quality of topic segmentation. Furthermore, we notice that prediction-based embedding\\nmethods improve topic segmentation.6. Conclusions\\nIn this paper, we investigated topic segmentation by using word embedding as representational basis. For this\\nreason we used the well known methods: LSA, Wor2Vec and GloVe. The aim of this study is to identify which method\\nis more eﬀective to learn word vector representations that provide the semantic meaning of words for both English\\nand Arabic languages. Yet, compared to other methods, Word2Vec is the most complicate one because of its di ﬀerent\\nmodels (CBOW and Skip-Gram) and approximation algorithms (negative sampling and hierarchical softmax). Forthis reason, we studied in depth Word2Vec by evaluating diﬀ erent combination of these models and algorithms in the\\ndomain of topic segmentation. As result, we showed that, independent from the used language, negative samplingis the most eﬃcient algorithms. Yet the choice of learning models is more delicate because it depends on the nature\\nof the training data. In fact, we found that CBOW is more e ﬃcient with frequent words. Yet Skip-Gram is more\\neﬃcient with infrequent words. Based on these results, we compared Word2Vec to LSA and GloVe. As results,we showed that Word2Vec and GloVe are more e ﬀective than LSA for both languages. Moreover, compared to\\nGloVe, Word2Vec presents the best word vector representations with a small dimensional semantic space. Besides,we showed that the quality of topic segmentation depends on the used language. In fact, for the Arabic language, the\\nquality of topic segmentation decreases compared to the English language because of its high complexity. Finally,\\ncompared to existent topic segmenters, we proved that ToSe-Word2Vec and ToSe-GloVe provide a high quality of\\ntopic segmentation. To go further, we will investigate the performance of LSA, Word2Vec and GloVe in other ﬁelds\\nsuch as topic analysis.\\nReferences\\n1.Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR,\\narXiv; 2013. p. 1301-3781.\\n2.Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP. 2014; 14:1532-1543.\\n3.Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association\\nfor Computational Linguistics, 2015; 3:211-225.\\n4.Mitra B. Vectorland: Brief Notes from Using Text Embeddings for Search, Search Solutions, Microsoft (Bing Sciences), 2015.\\n5.Altszyler E, Sigman M, and Slezak DF. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database.arXiv preprint arXiv:1610.01520; 2016.\\n6.Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.In ACL; 2014; 1:238-247.\\n7.Hearst MA. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 1997; 23(1):33-64.\\n8.Choi FYY. Advances in domain independent linear text, segmentation. Proceeding of NAACL, 2000;p.26-33.\\n9.Galley M, McKeown K, Fosler-Lussier E, Jing H. Discourse segmentation of multi-party conversation. In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics; 2003,1:562-569.\\n10.Ferret O. Improving text segmentation by combining endogenous and exogenous methods. International Conference of Recent Advances inNatural Language Processing (RANLP), Borovets, 2009; p.88-93\\n11.Reidl M, Beimann C. How text segmentation algorithms gain from topic models. NAACL-HLT Proceeding, Canada, 2012;p.553-557.\\n12.Naili M, Habacha AC, BenGhezala HH. Parameters driving e ﬀectiveness of LSA on topic segmentation, 17th International Conference on\\nIntelligent Text Processing and Computational Linguistics, Springer LNCS Series, Lecture Notes in Computer Science, Konya, 2016a.\\n13.Eisenstein J, and Barzilay R. Bayesian unsupervised topic segmentation. EMNLP, 2008;p.334-343.\\n14.Habacha AC, Naili N, Sammoud S. Topic segmentation for textual document written in Arabic language. KES-2014, 18th Annual Conference,\\nProcedia Computer Science, Gdynia, Poland; 2014:35:437-446.\\n15.Brants T, Chen F, Farahat A. Arabic Document Topic Analysis. LREC’02 Workshop on Arabic Language Resources and Evaluation. LasPalmas, Spain; 2002.\\n16.Touir AA, Makhtour H, and AlSanea W. Semantic-Based Segmentation of Arabic Texts. Inf. Tech. J., 2008;7(7):1009-1015.\\n17.Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R, Indexing by Latent Semantic Analysis. the American Society for Informa-\\ntion Science. 1990;41:391-407. Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 349\\nIf we compare our results with related works, some statements can be doubted. For example, Pennington et\\nal.2argue that GloVe performs better than Word2Vec in every task (word analogies, word similarity and named\\nentity recognition). Even Mitra4proved the same results. Yet, we share the same statement of Mikolov et al.1that\\nWord2Vec outperforms other models. Yet, the performance of GloVe is comparable to Word2Vec and they can be\\nconsidered as two powerful methods to learn word vector representations in the domain of topic segmentation. On the\\nother hand, if we compare modern prediction-based embeddings (Word2Vec and GloVe) with traditional count-based\\nmethods (LSA), we share the same claim as Baroni et al.6that LSA is less eﬃcient than the other models for synonym\\ndetection, concept categorization, selection preferences and analogy. Yet, Levy et al.3argue that LSA outperforms\\nWord2Vec and GloVe for word similarity. Moreover, Pennington et al.2argue that LSA outperforms Word2Vec but\\nnot GloVe for word analogy and similarity and named entity recognition tasks. Thus, we can explain these di ﬀerent\\nclaims by the choice of the domain application that inﬂuences the performance of each method.\\n5.3. Comparison with related works in topic segmentation\\nTo evaluate the performance of the proposed segmenters (ToSe-LSA, ToSA-Word2Vec and ToSe-GloVe), we con-\\nduct a comparison with related works in Tables 4 and 5. In fact, for each language (English and Arabic languages), we\\nconduct an evaluation of endogenous and exogenous segmenters on the same corpus. For the English language (Table\\n4), we can state that exogenous topic segmenters (BSeg, ToSe-LSA, ToSA-Word2Vec and ToSe-GloVe) are more\\neﬃcient than endogenous topic segmenters (C99, TextTiling, F06, LCseg and TopicTiling). This claim is explained\\nby the fact that adding external knowledge enhances the quality of topic segmentation. Nevertheless, the question is\\nwhich external resource that improves better the topic segmentation? In this study, we ﬁnd out that ToSe-LSA which\\nis based on a traditional count-based method has the biggest error rate. Yet the generative Bayesian model of Bseg13,\\nWord2Vec and GloVe are the most eﬃcient methods to use in topic segmentation especially Word2Vec. This can beexplained by the fact that probabilistic models and neural models can detect easily the semantic meaning of words\\nwhich is not the case for traditional methods such as LSA. For the Arabic language (Table 5), we only compared our\\nwork with the work of Habacha et al.\\n14. As shown in Table 5, we noticed that ArabTextTiling and ArabC99, which are\\nbased on endogenous approach, have the biggest error rate. Yet the performance of ToSe-LSA and ToSe-Word2Vec is\\nsimilar. Moreover, ToSe-GloVe is the most e ﬃcient segmenter.\\nTable 4. Comparison with existent English topic segmenters.\\nApproach English topic segmenter WindowDiﬀ\\nEndogenous C99818.92%\\nTextTiling728.99%\\nF061079.22%\\nLCseg913.32%\\nTopicTiling1130.46%\\nExogenous Bseg136.76%\\nToSe-LSA 8.14%\\nToSe-Word2Vec 5.79%\\nToSe-GloVe 5.94%\\nTable 5. Comparison with existent Arabic topic segmenters.\\nApproach Arabic topic segmenter WindowDiﬀ\\nEndogenous ArabC991436.10%\\nArabTextTiling1482.5%\\nExogenous ToSe-LSA 29.9%\\nToSe-Word2Vec 29.52%\\nToSe-GloVe 28.43%\\nBased on this evaluation, we can conclude that exogenous topic segmenters are much way better than endogenous\\ntopic segmenters for both Arabic and English languages. This can be explained by the fact that adding external\\nknowledge enhances the quality of topic segmentation. Furthermore, we notice that prediction-based embedding\\nmethods improve topic segmentation.6. Conclusions\\nIn this paper, we investigated topic segmentation by using word embedding as representational basis. For this\\nreason we used the well known methods: LSA, Wor2Vec and GloVe. The aim of this study is to identify which method\\nis more eﬀective to learn word vector representations that provide the semantic meaning of words for both English\\nand Arabic languages. Yet, compared to other methods, Word2Vec is the most complicate one because of its di ﬀerent\\nmodels (CBOW and Skip-Gram) and approximation algorithms (negative sampling and hierarchical softmax). Forthis reason, we studied in depth Word2Vec by evaluating diﬀ erent combination of these models and algorithms in the\\ndomain of topic segmentation. As result, we showed that, independent from the used language, negative samplingis the most eﬃcient algorithms. Yet the choice of learning models is more delicate because it depends on the nature\\nof the training data. In fact, we found that CBOW is more e ﬃcient with frequent words. Yet Skip-Gram is more\\neﬃcient with infrequent words. Based on these results, we compared Word2Vec to LSA and GloVe. As results,\\nwe showed that Word2Vec and GloVe are more e ﬀective than LSA for both languages. Moreover, compared to\\nGloVe, Word2Vec presents the best word vector representations with a small dimensional semantic space. Besides,we showed that the quality of topic segmentation depends on the used language. In fact, for the Arabic language, the\\nquality of topic segmentation decreases compared to the English language because of its high complexity. Finally,\\ncompared to existent topic segmenters, we proved that ToSe-Word2Vec and ToSe-GloVe provide a high quality of\\ntopic segmentation. To go further, we will investigate the performance of LSA, Word2Vec and GloVe in other ﬁelds\\nsuch as topic analysis.\\nReferences\\n1.Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR,\\narXiv; 2013. p. 1301-3781.\\n2.Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP. 2014; 14:1532-1543.\\n3.Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association\\nfor Computational Linguistics, 2015; 3:211-225.\\n4.Mitra B. Vectorland: Brief Notes from Using Text Embeddings for Search, Search Solutions, Microsoft (Bing Sciences), 2015.\\n5.Altszyler E, Sigman M, and Slezak DF. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database.arXiv preprint arXiv:1610.01520; 2016.\\n6.Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.In ACL; 2014; 1:238-247.\\n7.Hearst MA. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 1997; 23(1):33-64.\\n8.Choi FYY. Advances in domain independent linear text, segmentation. Proceeding of NAACL, 2000;p.26-33.\\n9.Galley M, McKeown K, Fosler-Lussier E, Jing H. Discourse segmentation of multi-party conversation. In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics; 2003,1:562-569.\\n10.Ferret O. Improving text segmentation by combining endogenous and exogenous methods. International Conference of Recent Advances inNatural Language Processing (RANLP), Borovets, 2009; p.88-93\\n11.Reidl M, Beimann C. How text segmentation algorithms gain from topic models. NAACL-HLT Proceeding, Canada, 2012;p.553-557.\\n12.Naili M, Habacha AC, BenGhezala HH. Parameters driving e ﬀectiveness of LSA on topic segmentation, 17th International Conference on\\nIntelligent Text Processing and Computational Linguistics, Springer LNCS Series, Lecture Notes in Computer Science, Konya, 2016a.\\n13.Eisenstein J, and Barzilay R. Bayesian unsupervised topic segmentation. EMNLP, 2008;p.334-343.\\n14.Habacha AC, Naili N, Sammoud S. Topic segmentation for textual document written in Arabic language. KES-2014, 18th Annual Conference,\\nProcedia Computer Science, Gdynia, Poland; 2014:35:437-446.\\n15.Brants T, Chen F, Farahat A. Arabic Document Topic Analysis. LREC’02 Workshop on Arabic Language Resources and Evaluation. LasPalmas, Spain; 2002.\\n16.Touir AA, Makhtour H, and AlSanea W. Semantic-Based Segmentation of Arabic Texts. Inf. Tech. J., 2008;7(7):1009-1015.\\n17.Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R, Indexing by Latent Semantic Analysis. the American Society for Informa-\\ntion Science. 1990;41:391-407.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64693ZIcfyN6"
   },
   "outputs": [],
   "source": [
    "# Access the appropriate signature for embedding\n",
    "embed_fn = elmo.signatures['default']\n",
    "\n",
    "# Embed the text using the embed_fn. Pass text as positional argument\n",
    "embeddings = embed_fn(tf.constant([text]))['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZz2SBxtgGpT",
    "outputId": "0a2e3e1f-3802-4dcd-eaf9-83dac8dcfe56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.5185611  -0.5409341   0.07054564 ...  0.78896505 -0.12693155\n",
      "   -0.33742332]\n",
      "  [-0.06468721  0.9545266   0.20303619 ... -0.37890467 -0.7762271\n",
      "   -0.41686863]\n",
      "  [ 0.09154911  0.17872748 -0.15596014 ... -0.5123843  -0.467335\n",
      "   -0.3965827 ]\n",
      "  ...\n",
      "  [-1.0815887  -0.80924284  0.43927002 ...  0.66765636 -0.23668984\n",
      "   -0.68361384]\n",
      "  [-0.7567288  -0.30398047 -0.2940536  ...  0.21035153  0.80204123\n",
      "   -1.4575136 ]\n",
      "  [-0.44408035  0.7996168  -0.7304639  ... -0.8548847  -0.16584936\n",
      "   -0.9443433 ]]], shape=(1, 11382, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8dbKUBk6lS_"
   },
   "outputs": [],
   "source": [
    "# advanced way to convert pdf into cleaned multiple chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "YdAiTLYHh9t5"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "\n",
    "    \"\"\"Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_file: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the extracted text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    with open(pdf_file, 'rb') as pdf_reader:\n",
    "        reader = PyPDF2.PdfReader(pdf_reader)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "NANSTAsB6-sJ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    \"\"\"Preprocesses text by removing punctuation, stop words, stemming, and lemmatization.\n",
    "\n",
    "    Args:\n",
    "        text: The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stem or lemmatize words\n",
    "    # Choose either stemming or lemmatization based on your preference\n",
    "    # stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "xZQP-B6w7Qte"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "\n",
    "    \"\"\"Chunks the text into smaller segments based on sentence boundaries.\n",
    "\n",
    "    Args:\n",
    "        text: The preprocessed text.\n",
    "        chunk_size: The desired maximum size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing a chunk and its starting index in the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    start_index = 0\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > chunk_size:\n",
    "            chunks.append((\" \".join(current_chunk), start_index))\n",
    "            current_chunk = []\n",
    "            start_index += len(\" \".join(current_chunk)) + 1  # Account for the space between sentences\n",
    "        current_chunk.append(sentence)\n",
    "    if current_chunk:\n",
    "        chunks.append((\" \".join(current_chunk), start_index))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "OIA027Ps7g6q"
   },
   "outputs": [],
   "source": [
    "def embed_pdf(pdf_file, elmo):\n",
    "\n",
    "    \"\"\"Embeds a PDF document using ELMO.\n",
    "\n",
    "    Args:\n",
    "        pdf_file: The path to the PDF file.\n",
    "        wv: The Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing a chunk embedding and its starting index in the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    chunks = chunk_text(text) # Pass the original 'text'\n",
    "\n",
    "    chunk_embeddings = []\n",
    "    for chunk, start_index in chunks:\n",
    "        chunk_tokens = preprocess_text(chunk)\n",
    "        # Use the 'elmo' object to generate embeddings\n",
    "        chunk_embedding = elmo.signatures[\"default\"](tf.constant([chunk]))['elmo']  # Assuming the model has a 'default' signature\n",
    "        chunk_embedding = np.mean(chunk_embedding, axis=0)  # Calculate the average vector\n",
    "\n",
    "        # Added a check if chunk_embedding is not None:\n",
    "        if chunk_embedding is not None:\n",
    "            chunk_embeddings.append((chunk_embedding, start_index))\n",
    "\n",
    "    return chunk_embeddings , chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "UlWTj9iV7l4K"
   },
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "\n",
    "pdf_file = \"/content/sample_data/Comparativestudyofwordembeddingalgorithm.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "lAhUu9dV7zME"
   },
   "outputs": [],
   "source": [
    "# Embed the PDF\n",
    "\n",
    "chunk_embeddings,chunks = embed_pdf(pdf_file, elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDqxFdoIOcMQ",
    "outputId": "aadcb862-0adc-413d-e503-5bab8c2ab459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j10uCH-zOhHF",
    "outputId": "cf2ccff1-b1a1-4d73-96d6-68f99f04f02e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 112 (2017)  340–349\\n1877-0509 © 2017 The Authors. Published by Elsevier B.V.\\nPeer-review under responsibility of KES International\\n10.1016/j.procs.2017.08.009\\n10.1016/j.procs.2017.08.009© 2017 The Authors. Published by Elsevier B.V .',\n",
       " 0)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQVR9vZIOlew",
    "outputId": "bf1e5874-dd0c-4a1b-924e-55ae22ed22bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Approach Arabic topic segmenter WindowDiﬀ\\nEndogenous ArabC991436.10%\\nArabTextTiling1482.5%\\nExogenous ToSe-LSA 29.9%\\nToSe-Word2Vec 29.52%\\nToSe-GloVe 28.43%\\nBased on this evaluation, we can conclude that exogenous topic segmenters are much way better than endogenous\\ntopic segmenters for both Arabic and English languages. This can be explained by the fact that adding external\\nknowledge enhances the quality of topic segmentation. Furthermore, we notice that prediction-based embedding\\nmethods improve topic segmentation.6. Conclusions\\nIn this paper, we investigated topic segmentation by using word embedding as representational basis. For this\\nreason we used the well known methods: LSA, Wor2Vec and GloVe. The aim of this study is to identify which method\\nis more eﬀective to learn word vector representations that provide the semantic meaning of words for both English\\nand Arabic languages. Yet, compared to other methods, Word2Vec is the most complicate one because of its di ﬀerent\\nmodels (CBOW and Skip-Gram) and approximation algorithms (negative sampling and hierarchical softmax). Forthis reason, we studied in depth Word2Vec by evaluating diﬀ erent combination of these models and algorithms in the\\ndomain of topic segmentation. As result, we showed that, independent from the used language, negative samplingis the most eﬃcient algorithms. Yet the choice of learning models is more delicate because it depends on the nature\\nof the training data. In fact, we found that CBOW is more e ﬃcient with frequent words. Yet Skip-Gram is more\\neﬃcient with infrequent words. Based on these results, we compared Word2Vec to LSA and GloVe. As results,\\nwe showed that Word2Vec and GloVe are more e ﬀective than LSA for both languages. Moreover, compared to\\nGloVe, Word2Vec presents the best word vector representations with a small dimensional semantic space. Besides,we showed that the quality of topic segmentation depends on the used language. In fact, for the Arabic language, the\\nquality of topic segmentation decreases compared to the English language because of its high complexity. Finally,\\ncompared to existent topic segmenters, we proved that ToSe-Word2Vec and ToSe-GloVe provide a high quality of\\ntopic segmentation. To go further, we will investigate the performance of LSA, Word2Vec and GloVe in other ﬁelds\\nsuch as topic analysis. References\\n1.Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR,\\narXiv; 2013. p. 1301-3781. 2.Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP. 2014; 14:1532-1543. 3.Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association\\nfor Computational Linguistics, 2015; 3:211-225. 4.Mitra B. Vectorland: Brief Notes from Using Text Embeddings for Search, Search Solutions, Microsoft (Bing Sciences), 2015. 5.Altszyler E, Sigman M, and Slezak DF. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database.arXiv preprint arXiv:1610.01520; 2016. 6.Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.In ACL; 2014; 1:238-247. 7.Hearst MA. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 1997; 23(1):33-64. 8.Choi FYY. Advances in domain independent linear text, segmentation. Proceeding of NAACL, 2000;p.26-33. 9.Galley M, McKeown K, Fosler-Lussier E, Jing H. Discourse segmentation of multi-party conversation. In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics; 2003,1:562-569. 10.Ferret O. Improving text segmentation by combining endogenous and exogenous methods. International Conference of Recent Advances inNatural Language Processing (RANLP), Borovets, 2009; p.88-93\\n11.Reidl M, Beimann C. How text segmentation algorithms gain from topic models. NAACL-HLT Proceeding, Canada, 2012;p.553-557. 12.Naili M, Habacha AC, BenGhezala HH. Parameters driving e ﬀectiveness of LSA on topic segmentation, 17th International Conference on\\nIntelligent Text Processing and Computational Linguistics, Springer LNCS Series, Lecture Notes in Computer Science, Konya, 2016a. 13.Eisenstein J, and Barzilay R. Bayesian unsupervised topic segmentation. EMNLP, 2008;p.334-343. 14.Habacha AC, Naili N, Sammoud S. Topic segmentation for textual document written in Arabic language. KES-2014, 18th Annual Conference,\\nProcedia Computer Science, Gdynia, Poland; 2014:35:437-446. 15.Brants T, Chen F, Farahat A. Arabic Document Topic Analysis. LREC’02 Workshop on Arabic Language Resources and Evaluation. LasPalmas, Spain; 2002. 16.Touir AA, Makhtour H, and AlSanea W. Semantic-Based Segmentation of Arabic Texts. Inf. Tech. J., 2008;7(7):1009-1015. 17.Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R, Indexing by Latent Semantic Analysis. the American Society for Informa-\\ntion Science. 1990;41:391-407.',\n",
       " 23)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0g8vGEHG76kt",
    "outputId": "3e702f34-de54-4ab6-d5b5-de0733bfdf84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxAp5AJeAacR",
    "outputId": "49ecf066-f037-42b4-fcd2-7cc857a9d417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5185611 , -0.5409341 ,  0.07054564, ...,  0.87855375,\n",
       "        -0.15627274, -0.33428386],\n",
       "       [-0.06468721,  0.9545266 ,  0.20303619, ..., -0.3320887 ,\n",
       "        -0.8282112 , -0.34949598],\n",
       "       [ 0.09154911,  0.17872748, -0.15596014, ..., -0.4465177 ,\n",
       "        -0.5123299 , -0.36933926],\n",
       "       ...,\n",
       "       [-0.2739171 , -0.2760446 ,  0.34321856, ..., -0.36861664,\n",
       "         0.21666196, -1.2448204 ],\n",
       "       [-0.32666162, -0.31426767, -0.35524338, ..., -0.11997335,\n",
       "        -0.49596033, -0.17764708],\n",
       "       [-0.5422448 , -0.38210356, -0.7529255 , ..., -0.080252  ,\n",
       "         0.03607737,  0.11280698]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jYsX6c4AqUy",
    "outputId": "569829cc-b631-459b-ab97-b81c96e85cf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18196136, -0.15308137,  0.35556495, ..., -0.00455654,\n",
       "        -0.20667076, -0.22041866],\n",
       "       [-0.38237062,  0.368627  ,  0.36678028, ..., -0.3826543 ,\n",
       "         0.48623466, -0.38074473],\n",
       "       [-0.14679319,  0.37798053, -0.07863915, ..., -0.19774854,\n",
       "         0.10006976,  0.02729857],\n",
       "       ...,\n",
       "       [-1.1159974 , -0.783383  ,  0.4140091 , ...,  0.6676561 ,\n",
       "        -0.23669007, -0.6836139 ],\n",
       "       [-0.725764  , -0.29312533, -0.30130523, ...,  0.21035105,\n",
       "         0.80204165, -1.4575149 ],\n",
       "       [-0.4280275 ,  0.8132723 , -0.7421303 , ..., -0.8548844 ,\n",
       "        -0.16584948, -0.94434357]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings[23][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBnH2w-6BOq_",
    "outputId": "5f23c906-2314-46a5-c5ec-aaf16bc833c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.5185611 , -0.5409341 ,  0.07054564, ...,  0.87855375,\n",
       "          -0.15627274, -0.33428386],\n",
       "         [-0.06468721,  0.9545266 ,  0.20303619, ..., -0.3320887 ,\n",
       "          -0.8282112 , -0.34949598],\n",
       "         [ 0.09154911,  0.17872748, -0.15596014, ..., -0.4465177 ,\n",
       "          -0.5123299 , -0.36933926],\n",
       "         ...,\n",
       "         [-0.2739171 , -0.2760446 ,  0.34321856, ..., -0.36861664,\n",
       "           0.21666196, -1.2448204 ],\n",
       "         [-0.32666162, -0.31426767, -0.35524338, ..., -0.11997335,\n",
       "          -0.49596033, -0.17764708],\n",
       "         [-0.5422448 , -0.38210356, -0.7529255 , ..., -0.080252  ,\n",
       "           0.03607737,  0.11280698]], dtype=float32),\n",
       "  0),\n",
       " (array([[-0.76515806, -0.05140942,  0.1016481 , ...,  0.5739049 ,\n",
       "           0.27230364, -0.465231  ],\n",
       "         [-0.630808  , -0.7205056 , -0.63053197, ..., -0.65991485,\n",
       "           0.3434443 , -0.07031652],\n",
       "         [-0.34732002, -0.6586375 , -0.33295983, ..., -0.61724716,\n",
       "           0.41304094, -0.38706958],\n",
       "         ...,\n",
       "         [-0.08622464,  0.3354862 ,  0.5018336 , ...,  0.31580085,\n",
       "          -0.08167178, -0.7689372 ],\n",
       "         [ 0.05133968,  0.17853147, -0.56634647, ...,  0.6333719 ,\n",
       "          -0.06196629, -0.08275188],\n",
       "         [-0.05521078,  0.4578866 , -0.5348036 , ...,  0.49408513,\n",
       "          -0.606125  ,  0.06973875]], dtype=float32),\n",
       "  1),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.1921097 ,\n",
       "          -0.16911836, -0.3021638 ],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43798766,\n",
       "           0.32634234,  0.02079748],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08617122,\n",
       "           0.3678538 , -0.88598096],\n",
       "         ...,\n",
       "         [-0.08763288,  0.33711058,  0.5016888 , ...,  0.3158012 ,\n",
       "          -0.08167155, -0.76893723],\n",
       "         [ 0.04882842,  0.17753729, -0.56318796, ...,  0.6333717 ,\n",
       "          -0.06196661, -0.08275204],\n",
       "         [-0.05533883,  0.45851326, -0.5346844 , ...,  0.49408522,\n",
       "          -0.6061249 ,  0.06973888]], dtype=float32),\n",
       "  2),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.1921097 ,\n",
       "          -0.16911836, -0.3021638 ],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43798766,\n",
       "           0.32634234,  0.02079748],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08617122,\n",
       "           0.3678538 , -0.88598096],\n",
       "         ...,\n",
       "         [-0.08763288,  0.33711058,  0.5016888 , ...,  0.3158012 ,\n",
       "          -0.08167155, -0.76893723],\n",
       "         [ 0.04882842,  0.17753729, -0.56318796, ...,  0.6333717 ,\n",
       "          -0.06196661, -0.08275204],\n",
       "         [-0.05533883,  0.45851326, -0.5346844 , ...,  0.49408522,\n",
       "          -0.6061249 ,  0.06973888]], dtype=float32),\n",
       "  3),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.19034423,\n",
       "          -0.16451257, -0.29674026],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43636948,\n",
       "           0.3287291 ,  0.02622595],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08236778,\n",
       "           0.36853516, -0.87978196],\n",
       "         ...,\n",
       "         [ 0.26194078,  0.12690715,  0.2383034 , ..., -0.00822219,\n",
       "           0.5129078 , -0.19507217],\n",
       "         [ 0.73814464, -0.58361185,  0.3086416 , ..., -0.44181374,\n",
       "           0.2430195 ,  0.5927671 ],\n",
       "         [-0.5531358 ,  0.3475899 , -0.11491857, ..., -0.4595777 ,\n",
       "           0.15090363, -0.13813503]], dtype=float32),\n",
       "  4),\n",
       " (array([[-0.5142068 , -0.46363464,  0.06954864, ..., -0.79952323,\n",
       "           0.08495528, -0.21757486],\n",
       "         [-1.2715001 , -0.75712526,  0.29971394, ..., -0.07138453,\n",
       "           0.43009377,  0.19614425],\n",
       "         [-0.36855036, -0.44768113, -0.14179529, ..., -0.3483438 ,\n",
       "           0.28412044,  0.17326686],\n",
       "         ...,\n",
       "         [-0.5028978 , -0.01127934, -0.02011991, ...,  0.0833414 ,\n",
       "          -0.1860182 , -0.6800531 ],\n",
       "         [-0.8249146 , -0.7409495 ,  0.33501592, ...,  0.29343495,\n",
       "           0.73179764,  0.14766243],\n",
       "         [-0.08738048,  0.7839496 , -0.11133431, ..., -0.70198095,\n",
       "           0.29771695,  0.00665477]], dtype=float32),\n",
       "  5),\n",
       " (array([[ 0.03341284,  0.09892435, -0.05410301, ...,  0.5621842 ,\n",
       "          -0.00646883, -0.6577543 ],\n",
       "         [-0.1678225 ,  0.02316928, -0.23741515, ...,  0.28497493,\n",
       "           0.19215259, -0.63160634],\n",
       "         [-0.0404743 , -0.25987318, -0.0110893 , ...,  0.1268669 ,\n",
       "           0.04843783, -1.0745232 ],\n",
       "         ...,\n",
       "         [-0.08644872,  0.33577538,  0.5019543 , ...,  0.31580105,\n",
       "          -0.08167166, -0.76893747],\n",
       "         [ 0.05023549,  0.17846045, -0.56505203, ...,  0.63337165,\n",
       "          -0.06196644, -0.08275227],\n",
       "         [-0.05544503,  0.4583614 , -0.53418905, ...,  0.49408525,\n",
       "          -0.60612464,  0.06973851]], dtype=float32),\n",
       "  6),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.19025819,\n",
       "          -0.16490239, -0.2976035 ],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43654603,\n",
       "           0.32833692,  0.02587041],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08267736,\n",
       "           0.3683111 , -0.87997377],\n",
       "         ...,\n",
       "         [-0.08658969,  0.33585426,  0.50201285, ...,  0.31580105,\n",
       "          -0.08167166, -0.76893747],\n",
       "         [ 0.04914264,  0.17761302, -0.563826  , ...,  0.63337165,\n",
       "          -0.06196644, -0.08275227],\n",
       "         [-0.05537814,  0.4584834 , -0.5336783 , ...,  0.49408525,\n",
       "          -0.60612464,  0.06973851]], dtype=float32),\n",
       "  7),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.19025819,\n",
       "          -0.16490239, -0.2976035 ],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43654603,\n",
       "           0.32833692,  0.02587041],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08267736,\n",
       "           0.3683111 , -0.87997377],\n",
       "         ...,\n",
       "         [-0.08658969,  0.33585426,  0.50201285, ...,  0.31580105,\n",
       "          -0.08167166, -0.76893747],\n",
       "         [ 0.04914264,  0.17761302, -0.563826  , ...,  0.63337165,\n",
       "          -0.06196644, -0.08275227],\n",
       "         [-0.05537814,  0.4584834 , -0.5336783 , ...,  0.49408525,\n",
       "          -0.60612464,  0.06973851]], dtype=float32),\n",
       "  8),\n",
       " (array([[ 0.55567   , -0.10357156, -0.09091133, ..., -0.19034423,\n",
       "          -0.16451257, -0.29674026],\n",
       "         [ 0.15349415, -0.1058057 , -0.0350397 , ..., -0.43636948,\n",
       "           0.3287291 ,  0.02622595],\n",
       "         [ 0.11167665, -0.1210684 ,  0.65949196, ..., -0.08236778,\n",
       "           0.36853516, -0.87978196],\n",
       "         ...,\n",
       "         [ 0.26194078,  0.12690715,  0.2383034 , ..., -0.00822219,\n",
       "           0.5129078 , -0.19507217],\n",
       "         [ 0.73814464, -0.58361185,  0.3086416 , ..., -0.44181374,\n",
       "           0.2430195 ,  0.5927671 ],\n",
       "         [-0.5531358 ,  0.3475899 , -0.11491857, ..., -0.4595777 ,\n",
       "           0.15090363, -0.13813503]], dtype=float32),\n",
       "  9),\n",
       " (array([[-0.5142068 , -0.46363464,  0.06954864, ..., -0.7994463 ,\n",
       "           0.08484656, -0.21726647],\n",
       "         [-1.2715001 , -0.75712526,  0.29971394, ..., -0.07095184,\n",
       "           0.43014327,  0.19631305],\n",
       "         [-0.36855036, -0.44768113, -0.14179529, ..., -0.34783027,\n",
       "           0.28389436,  0.17374296],\n",
       "         ...,\n",
       "         [ 0.18085955,  0.11156212,  0.1400983 , ..., -0.09140356,\n",
       "          -0.29778472,  0.3634361 ],\n",
       "         [-0.13597995,  0.09029064, -0.04744402, ...,  0.04294083,\n",
       "           0.04350182,  0.33362752],\n",
       "         [-0.10644592,  0.22721028, -0.5256511 , ..., -0.09152104,\n",
       "           0.1025952 ,  0.33638996]], dtype=float32),\n",
       "  10),\n",
       " (array([[-0.00646207,  0.00602164, -0.3559833 , ..., -0.21225739,\n",
       "          -0.14637765,  0.10418305],\n",
       "         [-0.08812702,  0.28225332, -0.11632455, ...,  0.21516554,\n",
       "          -0.08383127,  0.61456734],\n",
       "         [ 0.01514387, -0.5364462 ,  0.26073807, ..., -0.2728564 ,\n",
       "           0.3414318 ,  0.3173758 ],\n",
       "         ...,\n",
       "         [-0.3596067 ,  0.5289264 ,  0.0757332 , ...,  0.01670495,\n",
       "           0.43683848, -0.16736688],\n",
       "         [-0.22471407, -0.15585029,  0.11569776, ...,  0.4921832 ,\n",
       "           0.33429596, -0.25083688],\n",
       "         [-0.25194645,  0.56706583, -0.28545958, ...,  0.04915223,\n",
       "           0.36308753,  0.09281821]], dtype=float32),\n",
       "  11),\n",
       " (array([[ 0.20496741,  0.24360676, -0.17271775, ..., -0.6244912 ,\n",
       "          -0.10813843,  0.35072005],\n",
       "         [-0.16755016,  0.2108245 , -0.12410956, ..., -0.06841238,\n",
       "           0.673792  , -0.05960668],\n",
       "         [-1.0532553 , -0.0891735 ,  0.15110353, ...,  0.33275986,\n",
       "           0.23728037,  0.09823678],\n",
       "         ...,\n",
       "         [ 0.18342595,  0.10443873,  0.15449375, ..., -0.09140371,\n",
       "          -0.29778513,  0.36343616],\n",
       "         [-0.12052014,  0.09458187, -0.02870106, ...,  0.04294082,\n",
       "           0.04350176,  0.33362764],\n",
       "         [-0.09288017,  0.23068914, -0.5127735 , ..., -0.09152087,\n",
       "           0.10259521,  0.33638984]], dtype=float32),\n",
       "  12),\n",
       " (array([[-0.00646207,  0.00602164, -0.3559833 , ..., -0.21225739,\n",
       "          -0.14637765,  0.10418305],\n",
       "         [-0.08812702,  0.28225332, -0.11632455, ...,  0.21516554,\n",
       "          -0.08383127,  0.61456734],\n",
       "         [ 0.01514387, -0.5364462 ,  0.26073807, ..., -0.2728564 ,\n",
       "           0.3414318 ,  0.3173758 ],\n",
       "         ...,\n",
       "         [-0.3596067 ,  0.5289264 ,  0.0757332 , ...,  0.01670495,\n",
       "           0.43683848, -0.16736688],\n",
       "         [-0.22471407, -0.15585029,  0.11569776, ...,  0.4921832 ,\n",
       "           0.33429596, -0.25083688],\n",
       "         [-0.25194645,  0.56706583, -0.28545958, ...,  0.04915223,\n",
       "           0.36308753,  0.09281821]], dtype=float32),\n",
       "  13),\n",
       " (array([[ 0.20496741,  0.24360676, -0.17271775, ..., -0.6277081 ,\n",
       "          -0.11123168,  0.35067472],\n",
       "         [-0.16755016,  0.2108245 , -0.12410956, ..., -0.0679851 ,\n",
       "           0.67038083, -0.05900624],\n",
       "         [-1.0532553 , -0.0891735 ,  0.15110353, ...,  0.33343   ,\n",
       "           0.2340458 ,  0.0985524 ],\n",
       "         ...,\n",
       "         [ 0.2398336 ,  0.03507425, -0.54892826, ..., -1.2373519 ,\n",
       "          -0.18999772, -0.06933422],\n",
       "         [-0.29235   , -0.406133  ,  0.36205006, ...,  0.12809235,\n",
       "           0.42410025, -0.30947924],\n",
       "         [-0.29443082,  0.6574576 , -0.13414454, ..., -0.09176211,\n",
       "           0.22148854,  0.05022774]], dtype=float32),\n",
       "  14),\n",
       " (array([[-0.68619955, -1.1771777 , -1.0070524 , ..., -0.14457923,\n",
       "           0.1028439 , -0.5846949 ],\n",
       "         [-1.0416247 , -0.45658535,  0.28216764, ..., -0.0685581 ,\n",
       "           0.2078819 , -0.41194618],\n",
       "         [-0.3110351 , -0.4657299 ,  0.00619724, ..., -0.40066534,\n",
       "          -0.06726474, -0.9537258 ],\n",
       "         ...,\n",
       "         [-0.20899403,  0.0638938 ,  0.50381994, ...,  0.5262666 ,\n",
       "          -0.53402287, -0.17335887],\n",
       "         [ 0.1031589 ,  0.23620588,  0.4902689 , ..., -0.15217374,\n",
       "           0.54365855,  0.21018697],\n",
       "         [-0.52819854,  0.08086959, -0.47083253, ..., -0.6041499 ,\n",
       "           0.25221127,  0.00380304]], dtype=float32),\n",
       "  15),\n",
       " (array([[-0.17215881,  0.06200497,  0.03644013, ..., -0.63197607,\n",
       "           0.20653608, -0.24930713],\n",
       "         [-0.07377913, -0.04140854, -0.39608654, ..., -0.9539221 ,\n",
       "          -0.143747  , -0.4642163 ],\n",
       "         [-0.12677363, -0.23331243, -0.18753189, ...,  0.6204928 ,\n",
       "           0.52546406, -0.59000957],\n",
       "         ...,\n",
       "         [ 0.616459  ,  0.3641014 ,  0.3112492 , ...,  0.27051067,\n",
       "           0.5780479 , -0.49724066],\n",
       "         [ 0.04719485,  0.51099896,  0.20737188, ..., -0.36155716,\n",
       "           0.6462337 , -0.50946987],\n",
       "         [-0.37620515,  0.7442764 , -0.16507694, ..., -0.36541998,\n",
       "           0.70220166,  0.09558491]], dtype=float32),\n",
       "  16),\n",
       " (array([[-0.2029995 ,  0.27461612,  0.13320097, ..., -0.36820108,\n",
       "           0.704395  , -0.45795727],\n",
       "         [-0.5277144 ,  0.42886454, -0.07495932, ..., -0.1431652 ,\n",
       "           0.52961445,  0.03669548],\n",
       "         [-0.57144034,  0.11679117,  0.0300805 , ...,  0.38915783,\n",
       "           0.87707585,  0.05133039],\n",
       "         ...,\n",
       "         [ 0.17917667,  0.62821525, -0.25417978, ...,  0.15059279,\n",
       "           0.51043046,  0.67872   ],\n",
       "         [ 0.16329555,  0.7748526 ,  0.19557275, ...,  0.28471544,\n",
       "           0.17797074,  0.14069778],\n",
       "         [-0.23066722,  0.88668793, -0.3469147 , ...,  0.07516007,\n",
       "           1.2328646 , -0.40641984]], dtype=float32),\n",
       "  17),\n",
       " (array([[-0.01482869,  0.68962085, -0.18005306, ..., -0.01766136,\n",
       "          -0.16301596, -0.17921379],\n",
       "         [ 0.11732671, -0.03518492, -0.7030739 , ..., -0.0703761 ,\n",
       "          -0.2086539 , -0.34512866],\n",
       "         [-1.3026533 , -0.55113703, -0.49115235, ..., -0.14502499,\n",
       "           0.3354088 , -0.49306622],\n",
       "         ...,\n",
       "         [ 0.18162464,  0.626291  , -0.25548524, ...,  0.15059276,\n",
       "           0.5104303 ,  0.6787199 ],\n",
       "         [ 0.16802748,  0.7709129 ,  0.19648263, ...,  0.28471553,\n",
       "           0.17797093,  0.14069776],\n",
       "         [-0.22797695,  0.8837812 , -0.34616458, ...,  0.07516015,\n",
       "           1.2328644 , -0.40641966]], dtype=float32),\n",
       "  18),\n",
       " (array([[-0.01482869,  0.68962085, -0.18005306, ..., -0.00378941,\n",
       "          -0.04574762, -0.14477749],\n",
       "         [ 0.11732671, -0.03518492, -0.7030739 , ..., -0.01423162,\n",
       "          -0.09241246, -0.33653152],\n",
       "         [-1.3026533 , -0.55113703, -0.49115235, ..., -0.13299596,\n",
       "           0.46070394, -0.46727008],\n",
       "         ...,\n",
       "         [ 0.32098427,  0.22560263, -0.08694097, ..., -1.6538969 ,\n",
       "           0.2862929 , -0.2274697 ],\n",
       "         [ 0.16901341,  0.51424575,  0.15239842, ...,  0.11192472,\n",
       "          -0.2651421 ,  0.20545024],\n",
       "         [-0.13718411,  0.73355174, -0.33245584, ..., -0.21720415,\n",
       "           0.6805027 ,  0.2090145 ]], dtype=float32),\n",
       "  19),\n",
       " (array([[ 4.0649682e-01,  9.6205249e-02, -6.7331171e-01, ...,\n",
       "          -6.3113540e-01, -2.1335407e-01, -4.0228665e-04],\n",
       "         [-6.0343642e-02,  4.8573490e-02, -4.9127167e-01, ...,\n",
       "          -5.3691283e-02,  4.2715827e-01, -1.3418126e-01],\n",
       "         [-3.0693245e-01,  3.5270497e-01, -5.2676767e-02, ...,\n",
       "          -5.5013573e-01,  5.7998246e-01,  1.2589216e-01],\n",
       "         ...,\n",
       "         [-1.5589912e-01,  4.6399042e-01,  1.2450245e-01, ...,\n",
       "          -1.6298226e-01,  2.1819654e-01, -4.8296255e-01],\n",
       "         [-2.3825467e-03,  4.4567847e-01, -2.4565417e-01, ...,\n",
       "           6.6307589e-02,  8.5167736e-03,  7.5840041e-02],\n",
       "         [-2.6316726e-01,  8.6684477e-01, -6.6452891e-01, ...,\n",
       "           2.6640677e-01,  6.4480066e-01,  5.1444268e-01]], dtype=float32),\n",
       "  20),\n",
       " (array([[-0.18196136, -0.15308137,  0.35556495, ..., -0.01119897,\n",
       "          -0.18268442, -0.22321653],\n",
       "         [-0.38237062,  0.368627  ,  0.36678028, ..., -0.39189094,\n",
       "           0.49804285, -0.3891988 ],\n",
       "         [-0.14679319,  0.37798053, -0.07863915, ..., -0.2098047 ,\n",
       "           0.11431462,  0.00982361],\n",
       "         ...,\n",
       "         [ 0.30335885,  0.27720588, -0.11012789, ..., -1.6538963 ,\n",
       "           0.28629324, -0.22746965],\n",
       "         [ 0.17726679,  0.56010616,  0.1456542 , ...,  0.1119245 ,\n",
       "          -0.26514205,  0.20545009],\n",
       "         [-0.14412907,  0.7589239 , -0.3291497 , ..., -0.21720408,\n",
       "           0.6805029 ,  0.20901448]], dtype=float32),\n",
       "  21),\n",
       " (array([[ 4.0649682e-01,  9.6205249e-02, -6.7331171e-01, ...,\n",
       "          -6.3113540e-01, -2.1335407e-01, -4.0228665e-04],\n",
       "         [-6.0343642e-02,  4.8573490e-02, -4.9127167e-01, ...,\n",
       "          -5.3691283e-02,  4.2715827e-01, -1.3418126e-01],\n",
       "         [-3.0693245e-01,  3.5270497e-01, -5.2676767e-02, ...,\n",
       "          -5.5013573e-01,  5.7998246e-01,  1.2589216e-01],\n",
       "         ...,\n",
       "         [-1.5589912e-01,  4.6399042e-01,  1.2450245e-01, ...,\n",
       "          -1.6298226e-01,  2.1819654e-01, -4.8296255e-01],\n",
       "         [-2.3825467e-03,  4.4567847e-01, -2.4565417e-01, ...,\n",
       "           6.6307589e-02,  8.5167736e-03,  7.5840041e-02],\n",
       "         [-2.6316726e-01,  8.6684477e-01, -6.6452891e-01, ...,\n",
       "           2.6640677e-01,  6.4480066e-01,  5.1444268e-01]], dtype=float32),\n",
       "  22),\n",
       " (array([[-0.18196136, -0.15308137,  0.35556495, ..., -0.00455654,\n",
       "          -0.20667076, -0.22041866],\n",
       "         [-0.38237062,  0.368627  ,  0.36678028, ..., -0.3826543 ,\n",
       "           0.48623466, -0.38074473],\n",
       "         [-0.14679319,  0.37798053, -0.07863915, ..., -0.19774854,\n",
       "           0.10006976,  0.02729857],\n",
       "         ...,\n",
       "         [-1.1159974 , -0.783383  ,  0.4140091 , ...,  0.6676561 ,\n",
       "          -0.23669007, -0.6836139 ],\n",
       "         [-0.725764  , -0.29312533, -0.30130523, ...,  0.21035105,\n",
       "           0.80204165, -1.4575149 ],\n",
       "         [-0.4280275 ,  0.8132723 , -0.7421303 , ..., -0.8548844 ,\n",
       "          -0.16584948, -0.94434357]], dtype=float32),\n",
       "  23)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "RqCNbIzxEP0-"
   },
   "outputs": [],
   "source": [
    "def find_similar_paragraphs(query, chunk_embeddings, top_n=3):\n",
    "    \"\"\"Finds the most similar paragraphs to a query.\n",
    "\n",
    "    Args:\n",
    "        query: The query text.\n",
    "        chunk_embeddings: A list of tuples, each containing a chunk embedding and its starting index.\n",
    "        top_n: The number of most similar paragraphs to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing a similar paragraph and its starting index in the original text.\n",
    "    \"\"\"\n",
    "    # Convert the query to a tensor before passing it to the ELMo model\n",
    "    query_tensor = tf.constant([query])\n",
    "\n",
    "    # Pass the input as a dictionary to the SavedModel signature\n",
    "    query_embedding = elmo.signatures[\"default\"](text=query_tensor)[\"elmo\"].numpy()[0]\n",
    "\n",
    "    # Reshape embeddings to 2D by averaging along sequence length dimension before padding\n",
    "    # Modification: Ensure reshaped_embeddings is a 2D numpy array\n",
    "    # Correctly extract and reshape embeddings, assuming they are the first element of each tuple and are 3D\n",
    "    reshaped_embeddings = np.array([np.mean(embedding[0], axis=0) for embedding in chunk_embeddings])\n",
    "\n",
    "    # **Ensure query_embedding is 2D as well**\n",
    "    query_embedding = np.mean(query_embedding, axis=0).reshape(1, -1) # Reshape to (1, embedding_dim)\n",
    "\n",
    "    # Calculate similarities and get top indices using reshaped embeddings\n",
    "    # Ensure both inputs to cosine_similarity are 2D arrays\n",
    "    similarities = cosine_similarity(query_embedding, reshaped_embeddings)[0]  # Pass query_embedding directly\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "    return [chunk_embeddings[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDhMwNFXBVHi",
    "outputId": "d3477628-e346-4a8a-e114-5945883ed618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk starting at index 3: [[ 0.55567    -0.10357156 -0.09091133 ... -0.1921097  -0.16911836\n",
      "  -0.3021638 ]\n",
      " [ 0.15349415 -0.1058057  -0.0350397  ... -0.43798766  0.32634234\n",
      "   0.02079748]\n",
      " [ 0.11167665 -0.1210684   0.65949196 ... -0.08617122  0.3678538\n",
      "  -0.88598096]\n",
      " ...\n",
      " [-0.08763288  0.33711058  0.5016888  ...  0.3158012  -0.08167155\n",
      "  -0.76893723]\n",
      " [ 0.04882842  0.17753729 -0.56318796 ...  0.6333717  -0.06196661\n",
      "  -0.08275204]\n",
      " [-0.05533883  0.45851326 -0.5346844  ...  0.49408522 -0.6061249\n",
      "   0.06973888]]\n",
      "Chunk starting at index 2: [[ 0.55567    -0.10357156 -0.09091133 ... -0.1921097  -0.16911836\n",
      "  -0.3021638 ]\n",
      " [ 0.15349415 -0.1058057  -0.0350397  ... -0.43798766  0.32634234\n",
      "   0.02079748]\n",
      " [ 0.11167665 -0.1210684   0.65949196 ... -0.08617122  0.3678538\n",
      "  -0.88598096]\n",
      " ...\n",
      " [-0.08763288  0.33711058  0.5016888  ...  0.3158012  -0.08167155\n",
      "  -0.76893723]\n",
      " [ 0.04882842  0.17753729 -0.56318796 ...  0.6333717  -0.06196661\n",
      "  -0.08275204]\n",
      " [-0.05533883  0.45851326 -0.5346844  ...  0.49408522 -0.6061249\n",
      "   0.06973888]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "query = \"What are Word2Vectors?\"\n",
    "\n",
    "similar_chunks = find_similar_paragraphs(query, chunk_embeddings, top_n=2)\n",
    "for chunk_embedding, start_index in similar_chunks:\n",
    "    print(f\"Chunk starting at index {start_index}: {chunk_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziTWpDZ8EoVo",
    "outputId": "d90d8562-8383-48c2-9545-87e182ebd6d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model. Keywords: Word embedding, LSA, Word2Vec, GloVe, Topic segmentation. 1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3. However, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al. 2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel. : +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors.',\n",
       " 3)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHZLb8XoJOiF",
    "outputId": "4a3cd9bc-b808-4c66-b2e1-a381111067aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Published by Elsevier B.V.\\nPeer-review under responsibility of KES International.Available online at www.sciencedirect.com\\nProcedia Computer Science 00 (2017) 000–000\\nwww.elsevier.com/ locate /procedia\\nInternational Conference on Knowledge Based and Intelligent Information and Engineering\\nSystems, KES2017, 6-8 September 2017, Marseille, France\\nComparative study of word embedding methods in topic\\nsegmentation\\nMarwa Naili∗, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala\\nRIADI laboratory, National School of computer Science (ENSI),\\nUniversity of Mannouba 2010, Tunisia\\nAbstract\\nThe vector representations of words are very useful in di ﬀerent natural language processing tasks in order to capture the semantic\\nmeaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will\\nbe investigated in the ﬁeld of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth\\nby using diﬀerent models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on theused language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model. Keywords: Word embedding, LSA, Word2Vec, GloVe, Topic segmentation. 1. Introduction\\nOne of the interesting trends in natural language processing is the use of word embedding. The aim of this lat-\\nter is to build a low dimensional vector representation of word from a corpus of text. The main advantage of word\\nembedding is that it allows to oﬀer a more expressive and eﬃcient representation by maintaining the contextual sim-\\nilarity of words and by building a low dimensional vectors. Recently, the two well known methods for producing\\nword embedding models are Word2Vec1and Global Vectors GloVe2. These two methods have been drawing great\\nattention and it has been reported to be the most eﬃcient ones for learning vector representations of words1,2. For this\\nreason, Word2Vec and Glove have been used in diﬀ erent natural language processing tasks such as Word Similarity3. However, it is diﬃcult to choose one of these two methods. In fact, Pennington et al.2proved that GloVe is more\\neﬃcient than Word2Vec. Furthermore, they proved that classical methods can be more useful than Word2Vec in par-ticular the Latent Semantic Analysis (LSA). This technique is considered as one of the most inﬂuential early models\\nfor word embedding. According to Pennington et al. 2, this can be explained by the fact that Word2Vec learns low\\ndimensional vectors from the start and it does not use all the information from the training corpus. Mitra4shared the\\nsame concern as Pennington et al.2with the following question: ”What if I told you that everyone who uses Word2vec\\nis throwing half the model away?”. Furthermore, Altszyler et al.5proved that the performance of Word2Vec to detect\\nsemantic words relations decreases when the corpus size is reduced. Yet they proved that LSA is more stable and it is\\n∗Corresponding author. Tel. : +216-50-765-809 ;\\nE-mail address: maroua.naili@riadi.rnu.tn\\n1877-0509 c⃝2017 The Authors.',\n",
       " 2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYNRO2kbO-1e",
    "outputId": "6242e370-c9bd-4108-8e9b-38edfe9204ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk starting at index 21: [[-0.18196136 -0.15308137  0.35556495 ... -0.01119897 -0.18268442\n",
      "  -0.22321653]\n",
      " [-0.38237062  0.368627    0.36678028 ... -0.39189094  0.49804285\n",
      "  -0.3891988 ]\n",
      " [-0.14679319  0.37798053 -0.07863915 ... -0.2098047   0.11431462\n",
      "   0.00982361]\n",
      " ...\n",
      " [ 0.30335885  0.27720588 -0.11012789 ... -1.6538963   0.28629324\n",
      "  -0.22746965]\n",
      " [ 0.17726679  0.56010616  0.1456542  ...  0.1119245  -0.26514205\n",
      "   0.20545009]\n",
      " [-0.14412907  0.7589239  -0.3291497  ... -0.21720408  0.6805029\n",
      "   0.20901448]]\n",
      "Chunk starting at index 23: [[-0.18196136 -0.15308137  0.35556495 ... -0.00455654 -0.20667076\n",
      "  -0.22041866]\n",
      " [-0.38237062  0.368627    0.36678028 ... -0.3826543   0.48623466\n",
      "  -0.38074473]\n",
      " [-0.14679319  0.37798053 -0.07863915 ... -0.19774854  0.10006976\n",
      "   0.02729857]\n",
      " ...\n",
      " [-1.1159974  -0.783383    0.4140091  ...  0.6676561  -0.23669007\n",
      "  -0.6836139 ]\n",
      " [-0.725764   -0.29312533 -0.30130523 ...  0.21035105  0.80204165\n",
      "  -1.4575149 ]\n",
      " [-0.4280275   0.8132723  -0.7421303  ... -0.8548844  -0.16584948\n",
      "  -0.94434357]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "query = \"Evaluation of Word2Vec parameters\"\n",
    "\n",
    "similar_chunks = find_similar_paragraphs(query, chunk_embeddings, top_n=2)\n",
    "for chunk_embedding, start_index in similar_chunks:\n",
    "    print(f\"Chunk starting at index {start_index}: {chunk_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAQsSCQZPmxq",
    "outputId": "c3ef205b-e726-4a24-e176-718ae3539a0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Approach Arabic topic segmenter WindowDiﬀ\\nEndogenous ArabC991436.10%\\nArabTextTiling1482.5%\\nExogenous ToSe-LSA 29.9%\\nToSe-Word2Vec 29.52%\\nToSe-GloVe 28.43%\\nBased on this evaluation, we can conclude that exogenous topic segmenters are much way better than endogenous\\ntopic segmenters for both Arabic and English languages. This can be explained by the fact that adding externalknowledge enhances the quality of topic segmentation. Furthermore, we notice that prediction-based embedding\\nmethods improve topic segmentation.6. Conclusions\\nIn this paper, we investigated topic segmentation by using word embedding as representational basis. For this\\nreason we used the well known methods: LSA, Wor2Vec and GloVe. The aim of this study is to identify which method\\nis more eﬀective to learn word vector representations that provide the semantic meaning of words for both English\\nand Arabic languages. Yet, compared to other methods, Word2Vec is the most complicate one because of its di ﬀerent\\nmodels (CBOW and Skip-Gram) and approximation algorithms (negative sampling and hierarchical softmax). Forthis reason, we studied in depth Word2Vec by evaluating diﬀ erent combination of these models and algorithms in the\\ndomain of topic segmentation. As result, we showed that, independent from the used language, negative samplingis the most eﬃcient algorithms. Yet the choice of learning models is more delicate because it depends on the nature\\nof the training data. In fact, we found that CBOW is more e ﬃcient with frequent words. Yet Skip-Gram is more\\neﬃcient with infrequent words. Based on these results, we compared Word2Vec to LSA and GloVe. As results,we showed that Word2Vec and GloVe are more e ﬀective than LSA for both languages. Moreover, compared to\\nGloVe, Word2Vec presents the best word vector representations with a small dimensional semantic space. Besides,we showed that the quality of topic segmentation depends on the used language. In fact, for the Arabic language, the\\nquality of topic segmentation decreases compared to the English language because of its high complexity. Finally,\\ncompared to existent topic segmenters, we proved that ToSe-Word2Vec and ToSe-GloVe provide a high quality of\\ntopic segmentation. To go further, we will investigate the performance of LSA, Word2Vec and GloVe in other ﬁelds\\nsuch as topic analysis. References\\n1.Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR,\\narXiv; 2013. p. 1301-3781. 2.Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP. 2014; 14:1532-1543. 3.Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association\\nfor Computational Linguistics, 2015; 3:211-225. 4.Mitra B. Vectorland: Brief Notes from Using Text Embeddings for Search, Search Solutions, Microsoft (Bing Sciences), 2015. 5.Altszyler E, Sigman M, and Slezak DF. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database.arXiv preprint arXiv:1610.01520; 2016. 6.Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.In ACL; 2014; 1:238-247. 7.Hearst MA. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 1997; 23(1):33-64. 8.Choi FYY. Advances in domain independent linear text, segmentation. Proceeding of NAACL, 2000;p.26-33. 9.Galley M, McKeown K, Fosler-Lussier E, Jing H. Discourse segmentation of multi-party conversation. In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics; 2003,1:562-569. 10.Ferret O. Improving text segmentation by combining endogenous and exogenous methods. International Conference of Recent Advances inNatural Language Processing (RANLP), Borovets, 2009; p.88-93\\n11.Reidl M, Beimann C. How text segmentation algorithms gain from topic models. NAACL-HLT Proceeding, Canada, 2012;p.553-557. 12.Naili M, Habacha AC, BenGhezala HH. Parameters driving e ﬀectiveness of LSA on topic segmentation, 17th International Conference on\\nIntelligent Text Processing and Computational Linguistics, Springer LNCS Series, Lecture Notes in Computer Science, Konya, 2016a. 13.Eisenstein J, and Barzilay R. Bayesian unsupervised topic segmentation. EMNLP, 2008;p.334-343. 14.Habacha AC, Naili N, Sammoud S. Topic segmentation for textual document written in Arabic language. KES-2014, 18th Annual Conference,\\nProcedia Computer Science, Gdynia, Poland; 2014:35:437-446. 15.Brants T, Chen F, Farahat A. Arabic Document Topic Analysis. LREC’02 Workshop on Arabic Language Resources and Evaluation. LasPalmas, Spain; 2002. 16.Touir AA, Makhtour H, and AlSanea W. Semantic-Based Segmentation of Arabic Texts. Inf. Tech. J., 2008;7(7):1009-1015. 17.Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R, Indexing by Latent Semantic Analysis. the American Society for Informa-\\ntion Science. 1990;41:391-407. Marwa Naili et al. / Procedia Computer Science 112 (2017) 340–349 349\\nIf we compare our results with related works, some statements can be doubted. For example, Pennington et\\nal.2argue that GloVe performs better than Word2Vec in every task (word analogies, word similarity and named\\nentity recognition). Even Mitra4proved the same results. Yet, we share the same statement of Mikolov et al.1that\\nWord2Vec outperforms other models. Yet, the performance of GloVe is comparable to Word2Vec and they can be\\nconsidered as two powerful methods to learn word vector representations in the domain of topic segmentation.',\n",
       " 21)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5cNgwuSPrUX",
    "outputId": "907ac70f-d3d6-41bb-c881-4d29b67f9fed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Approach Arabic topic segmenter WindowDiﬀ\\nEndogenous ArabC991436.10%\\nArabTextTiling1482.5%\\nExogenous ToSe-LSA 29.9%\\nToSe-Word2Vec 29.52%\\nToSe-GloVe 28.43%\\nBased on this evaluation, we can conclude that exogenous topic segmenters are much way better than endogenous\\ntopic segmenters for both Arabic and English languages. This can be explained by the fact that adding external\\nknowledge enhances the quality of topic segmentation. Furthermore, we notice that prediction-based embedding\\nmethods improve topic segmentation.6. Conclusions\\nIn this paper, we investigated topic segmentation by using word embedding as representational basis. For this\\nreason we used the well known methods: LSA, Wor2Vec and GloVe. The aim of this study is to identify which method\\nis more eﬀective to learn word vector representations that provide the semantic meaning of words for both English\\nand Arabic languages. Yet, compared to other methods, Word2Vec is the most complicate one because of its di ﬀerent\\nmodels (CBOW and Skip-Gram) and approximation algorithms (negative sampling and hierarchical softmax). Forthis reason, we studied in depth Word2Vec by evaluating diﬀ erent combination of these models and algorithms in the\\ndomain of topic segmentation. As result, we showed that, independent from the used language, negative samplingis the most eﬃcient algorithms. Yet the choice of learning models is more delicate because it depends on the nature\\nof the training data. In fact, we found that CBOW is more e ﬃcient with frequent words. Yet Skip-Gram is more\\neﬃcient with infrequent words. Based on these results, we compared Word2Vec to LSA and GloVe. As results,\\nwe showed that Word2Vec and GloVe are more e ﬀective than LSA for both languages. Moreover, compared to\\nGloVe, Word2Vec presents the best word vector representations with a small dimensional semantic space. Besides,we showed that the quality of topic segmentation depends on the used language. In fact, for the Arabic language, the\\nquality of topic segmentation decreases compared to the English language because of its high complexity. Finally,\\ncompared to existent topic segmenters, we proved that ToSe-Word2Vec and ToSe-GloVe provide a high quality of\\ntopic segmentation. To go further, we will investigate the performance of LSA, Word2Vec and GloVe in other ﬁelds\\nsuch as topic analysis. References\\n1.Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR,\\narXiv; 2013. p. 1301-3781. 2.Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP. 2014; 14:1532-1543. 3.Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association\\nfor Computational Linguistics, 2015; 3:211-225. 4.Mitra B. Vectorland: Brief Notes from Using Text Embeddings for Search, Search Solutions, Microsoft (Bing Sciences), 2015. 5.Altszyler E, Sigman M, and Slezak DF. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database.arXiv preprint arXiv:1610.01520; 2016. 6.Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.In ACL; 2014; 1:238-247. 7.Hearst MA. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 1997; 23(1):33-64. 8.Choi FYY. Advances in domain independent linear text, segmentation. Proceeding of NAACL, 2000;p.26-33. 9.Galley M, McKeown K, Fosler-Lussier E, Jing H. Discourse segmentation of multi-party conversation. In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics; 2003,1:562-569. 10.Ferret O. Improving text segmentation by combining endogenous and exogenous methods. International Conference of Recent Advances inNatural Language Processing (RANLP), Borovets, 2009; p.88-93\\n11.Reidl M, Beimann C. How text segmentation algorithms gain from topic models. NAACL-HLT Proceeding, Canada, 2012;p.553-557. 12.Naili M, Habacha AC, BenGhezala HH. Parameters driving e ﬀectiveness of LSA on topic segmentation, 17th International Conference on\\nIntelligent Text Processing and Computational Linguistics, Springer LNCS Series, Lecture Notes in Computer Science, Konya, 2016a. 13.Eisenstein J, and Barzilay R. Bayesian unsupervised topic segmentation. EMNLP, 2008;p.334-343. 14.Habacha AC, Naili N, Sammoud S. Topic segmentation for textual document written in Arabic language. KES-2014, 18th Annual Conference,\\nProcedia Computer Science, Gdynia, Poland; 2014:35:437-446. 15.Brants T, Chen F, Farahat A. Arabic Document Topic Analysis. LREC’02 Workshop on Arabic Language Resources and Evaluation. LasPalmas, Spain; 2002. 16.Touir AA, Makhtour H, and AlSanea W. Semantic-Based Segmentation of Arabic Texts. Inf. Tech. J., 2008;7(7):1009-1015. 17.Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R, Indexing by Latent Semantic Analysis. the American Society for Informa-\\ntion Science. 1990;41:391-407.',\n",
       " 23)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm-v3fYaQEvn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
